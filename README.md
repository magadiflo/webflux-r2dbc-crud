# [WebFlux R2DBC Crud using PostgreSQL](https://www.youtube.com/watch?v=s6qKE0FD3BU&t=2137s)

- Proyecto tomado del canal de youtube de `Joas Dev`.
- Este proyecto est√° actualizado al `25/06/2025` con algunos detalles que se vieron en el curso de
  [java-reactive-programming](https://github.com/magadiflo/java-reactive-programming.git) y de
  [webFlux-masterclass-microservices](https://github.com/magadiflo/webFlux-masterclass-microservices.git).

---

## Dependencias

A continuaci√≥n se muestran las dependencias utilizadas en este proyecto, de las cuales, la √∫nica dependencia que
agregamos manualmente fue [MapStruct](https://mapstruct.org/), las dem√°s dependencias las agregamos desde
[Spring Initializr (ver dependencias)](https://start.spring.io/#!type=maven-project&language=java&platformVersion=3.5.3&packaging=jar&jvmVersion=21&groupId=dev.magadiflo&artifactId=webflux-r2dbc-crud&name=webflux-r2dbc-crud&description=Demo%20project%20for%20Spring%20WebFlux%20with%20r2dbc%20PostgreSQL&packageName=dev.magadiflo.r2dbc.app&dependencies=webflux,data-r2dbc,postgresql,lombok).

````xml
<!--Spring Boot 3.5.3-->
<!--Java 21-->
<!--org.mapstruct.version 1.6.3-->
<!--lombok-mapstruct-binding.version 0.2.0-->
<dependencies>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-data-r2dbc</artifactId>
    </dependency>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-validation</artifactId>
    </dependency>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-webflux</artifactId>
    </dependency>

    <!--Agregado manualmente-->
    <dependency>
        <groupId>org.mapstruct</groupId>
        <artifactId>mapstruct</artifactId>
        <version>${org.mapstruct.version}</version>
    </dependency>
    <!--/Agregado manualmente-->
    <dependency>
        <groupId>org.postgresql</groupId>
        <artifactId>postgresql</artifactId>
        <scope>runtime</scope>
    </dependency>
    <dependency>
        <groupId>org.postgresql</groupId>
        <artifactId>r2dbc-postgresql</artifactId>
        <scope>runtime</scope>
    </dependency>
    <dependency>
        <groupId>org.projectlombok</groupId>
        <artifactId>lombok</artifactId>
        <optional>true</optional>
    </dependency>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-test</artifactId>
        <scope>test</scope>
    </dependency>
    <dependency>
        <groupId>io.projectreactor</groupId>
        <artifactId>reactor-test</artifactId>
        <scope>test</scope>
    </dependency>
</dependencies>
````

### Procesadores de anotaciones

- [Using MapStruct with Maven and Lombok.](https://bootify.io/spring-data/mapstruct-with-maven-and-lombok.html)
- [Using MapStruct With Lombok](https://www.baeldung.com/java-mapstruct-lombok)

Como vamos a trabajar con `MapStruct` necesitamos ampliar el `maven-compiler-plugin` para activar la generaci√≥n de
c√≥digo de `MapStruct`. Observar que nuestro primer procesador de anotaciones es `Lombok`, seguido directamente por
`MapStruct`. Se requiere otra referencia a `lombok-mapstruct-binding` para que estas dos bibliotecas funcionen juntas.
Sin `Lombok`, solo se necesitar√≠a el `mapstruct-processor` en este momento.

````xml

<plugins>
    <!--MapStruct-->
    <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-compiler-plugin</artifactId>
        <version>${maven-compiler-plugin.version}</version>
        <configuration>
            <source>${java.version}</source>
            <target>${java.version}</target>
            <annotationProcessorPaths>
                <path>
                    <groupId>org.projectlombok</groupId>
                    <artifactId>lombok</artifactId>
                    <version>${lombok.version}</version>
                </path>
                <path>
                    <groupId>org.mapstruct</groupId>
                    <artifactId>mapstruct-processor</artifactId>
                    <version>${org.mapstruct.version}</version>
                </path>
                <path>
                    <groupId>org.projectlombok</groupId>
                    <artifactId>lombok-mapstruct-binding</artifactId>
                    <version>${lombok-mapstruct-binding.version}</version>
                </path>
            </annotationProcessorPaths>
        </configuration>
    </plugin>
    <!--/MapStruct-->
</plugins>
````

Es f√°cil cometer errores aqu√≠, ya que los procesadores de anotaciones son una funci√≥n avanzada. El principal error es
olvidar que nuestro entorno de ejecuci√≥n buscar√° procesadores de anotaciones en el `path` o en el `classpath`, pero no
en ambas.

Debemos tener en cuenta que, a partir de la versi√≥n `1.18.16` de `Lombok` hacia arriba, necesitamos agregar tanto la
dependencia `lombok-mapstruct-binding` de `Lombok` como la dependencia `mapstruct-processor` en el elemento
`annotationProcessorPaths`. Si no lo hacemos, podr√≠amos obtener un error de compilaci√≥n:
`‚ÄúPropiedad desconocida en el tipo de resultado‚Ä¶‚Äù`.

Necesitamos la dependencia `lombok-mapstruct-binding` para que `Lombok` y `MapStruct` funcionen juntos. En esencia,
le indica a `MapStruct` que espere hasta que `Lombok` haya completado todo el procesamiento de anotaciones antes
de generar clases de mapeador para los beans mejorados con `Lombok`.

## MapStruct vs ModelMapper

> En el proyecto original del tutorial de `Joas Dev` se usa como mapeador a `ModelMapper`, pero en mi caso, opt√© por
> usar `MapStruct` debido a las siguientes razones.

### ‚öôÔ∏è MapStruct

- `Rendimiento superior`: Genera c√≥digo de mapeo en tiempo de compilaci√≥n, `evitando el uso de reflexi√≥n` y mejorando
  la velocidad.
- `Detecci√≥n temprana de errores`: Al generar c√≥digo en compilaci√≥n, los errores de mapeo se detectan antes de ejecutar
  la aplicaci√≥n.
- `Menor uso de memoria`: No usa reflection, lo que reduce la presi√≥n sobre el garbage collector.
- `Ideal para proyectos grandes o cr√≠ticos en rendimiento`, como los que suelen construirse con `WebFlux`.
- `M√°s configuraci√≥n inicial`, ya que debes definir interfaces y m√©todos de mapeo expl√≠citamente.

### üîÑ ModelMapper

- `M√°s f√°cil de usar al principio`: Mapea autom√°ticamente campos con nombres similares `usando reflexi√≥n`.
- Puede tener problemas con tipos gen√©ricos complejos de los streams reactivos
- `Menor esfuerzo inicial`, √∫til para prototipos o proyectos peque√±os.
- `Menor rendimiento`: La `reflexi√≥n` introduce una sobrecarga que puede ser significativa en aplicaciones reactivas.
- `Menos control` sobre el proceso de mapeo, lo que puede dificultar el mantenimiento a largo plazo.
- Los errores de configuraci√≥n solo se detectan en runtime.
- Mayor consumo de memoria y CPU.

### ‚úÖ Conclusi√≥n

Dado el enfoque en rendimiento, claridad y mantenimiento en sistemas reactivamente transaccionales, `MapStruct` encaja
mejor con el estilo y los objetivos. Adem√°s, al generar c√≥digo expl√≠cito, facilita la documentaci√≥n y el control del
flujo de datos.

## Creando base de datos

Creamos la base de datos `db_webflux_r2dbc` cuyo usuario es `postgres` y la contrase√±a es `magadiflo`.

![01.png](assets/01.png)

## Configurando base de datos

En el `application.yml` agregamos las siguientes configuraciones.

````yml
server:
  port: 8080
  error:
    include-message: always

spring:
  application:
    name: webflux-r2dbc-crud
  r2dbc:
    url: r2dbc:postgresql://localhost:5432/db_webflux_r2dbc
    username: postgres
    password: magadiflo
````

La URL de conexi√≥n est√° configurada en `r2dbc:postgresql://localhost:5432/db_webflux_r2dbc`, donde:

- `r2dbc`, indicamos que usaremos `r2dbc` para conectarnos a la base de datos. Recordar que cuando usamos una
  aplicaci√≥n tradicional como `Spring Boot MVC` (no reactiva) usamos `jdbc`.
- `db_webflux_r2dbc`, es el nombre de la base de datos.
- `5432`, es el puerto por defecto `PostgreSQL`.

Las propiedades `spring.r2dbc.username` y `spring.r2dbc.password` proporcionan las credenciales para conectarse a
la base de datos.

## Habilitando logging para ver los queries y parameters en las consultas a PostgreSQL

En el `application.yml` agregamos las siguientes configuraciones para poder observar qu√© instrucciones sql se est√°n
ejecutando y qu√© par√°metros se est√°n enviando.

````yml
logging:
  level:
    io.r2dbc.postgresql.QUERY: DEBUG
    io.r2dbc.postgresql.PARAM: DEBUG
````

‚ú® ¬øQu√© hace cada l√≠nea?

- `logging.level`, define el nivel de detalle de los logs que Spring Boot va a registrar en la consola o en el archivo
  de logs, por cada paquete o clase.
- `io.r2dbc.postgresql.QUERY: DEBUG`, este ajuste activa el registro en nivel DEBUG de todas las consultas SQL que se
  ejecutan a trav√©s del driver `R2DBC PostgreSQL`.
    - Te permite ver qu√© sentencias SQL se env√≠an a la base de datos de manera reactiva.
    - Ejemplo de log que ver√°s:
        ````bash
        DEBUG 17820 --- io.r2dbc.postgresql.QUERY: Executing query: INSERT INTO authors (first_name, last_name, birthdate) VALUES ($1, $2, $3) RETURNING id
        ````
- `io.r2dbc.postgresql.PARAM: DEBUG`, este ajuste muestra en nivel DEBUG los valores de los par√°metros que se usan en
  las consultas preparadas.
    - Es √∫til para depurar y saber exactamente qu√© datos se est√°n pasando en las variables.
    - Ejemplo de log que ver√°s:
      ````bash
      DEBUG 17820 --- io.r2dbc.postgresql.PARAM: Bind parameter [0] to: Ale
      DEBUG 17820 --- io.r2dbc.postgresql.PARAM: Bind parameter [1] to: Flo
      DEBUG 17820 --- io.r2dbc.postgresql.PARAM: Bind parameter [2] to: 2025-05-06
      ````

## Creando el esquema y datos de nuestra base de datos

Creamos el siguiente directorio `src/main/resources/sql` en nuestro proyecto. Aqu√≠ creamos el archivo `scheme.sql`
donde definimos las tablas `authors`, `books` y su relaci√≥n de muchos a muchos `book_authors`.

Definimos las instrucciones `DROP TABLE...` al inicio de este script para que la aplicaci√≥n inicie limpia y siempre
con los datos iniciales del archivo `data.sql` que crearemos luego.

````sql
DROP TABLE IF EXISTS book_authors;
DROP TABLE IF EXISTS authors;
DROP TABLE IF EXISTS books;

CREATE TABLE authors(
    id SERIAL,
    first_name VARCHAR(45) NOT NULL,
    last_name VARCHAR(45) NOT NULL,
    birthdate DATE NOT NULL,
    CONSTRAINT pk_authors PRIMARY KEY(id)
);

CREATE TABLE books(
    id SERIAL,
    title VARCHAR(255) NOT NULL,
    publication_date DATE NOT NULL,
    online_availability BOOLEAN DEFAULT FALSE,
    CONSTRAINT pk_books PRIMARY KEY(id)
);

CREATE TABLE book_authors(
    book_id INTEGER NOT NULL,
    author_id INTEGER NOT NULL,
    CONSTRAINT fk_books_book_authors FOREIGN KEY(book_id) REFERENCES books(id),
    CONSTRAINT fk_authors_book_authors FOREIGN KEY(author_id) REFERENCES authors(id),
    CONSTRAINT pk_book_authors PRIMARY KEY(book_id, author_id)
);
````

Creamos el script `data.sql` en el mismo directorio que el `scheme.sql`. En este script `data.sql` vamos a definir
los valores iniciales con las que iniciar√° nuestra aplicaci√≥n cada vez que sea ejecutada.

````sql
INSERT INTO authors(first_name, last_name, birthdate)
VALUES
('Milagros', 'D√≠az', '2006-06-15'),
('Lesly', '√Åguila', '1995-06-09'),
('Kiara', 'Lozano', '2001-10-03'),
('Briela', 'Cirilo', '1997-09-25');

INSERT INTO books(title, publication_date, online_availability)
VALUES
('Cien a√±os de soledad', '1999-01-15', true),
('Paco Yunque', '1985-03-18', true),
('Los perros hambrientos', '2002-05-06', false),
('Edipo Rey', '1988-07-15', true);

-- Book 1: tiene como author a Milagros y Lesly
INSERT INTO book_authors(book_id, author_id)
VALUES
(1, 1),
(1, 2);

-- Book 2: tiene como author a Kiara
INSERT INTO book_authors(book_id, author_id)
VALUES
(2, 3);
````

## Inicializando Scheme y Data

Crearemos una clase de configuraci√≥n en `dev/magadiflo/r2dbc/app/config/DatabaseInitConfig.java` donde definiremos un
`@Bean` que nos retornar√° un objeto del tipo `ConnectionFactoryInitializer`.

`Spring Data R2DBC` `ConnectionFactoryInitializer` proporciona una manera conveniente de configurar e inicializar una
f√°brica de conexiones para una conexi√≥n de base de datos reactiva en una aplicaci√≥n `Spring`. Escanear√° el
`scheme.sql` y el `data.sql` en el `classpath` y ejecutar√° los `scripts SQL` para inicializar la base de datos cuando
la base de datos est√© conectada.

````java

@Configuration
public class DatabaseInitConfig {
    @Bean
    public ConnectionFactoryInitializer initializer(ConnectionFactory connectionFactory) {
        ClassPathResource schema = new ClassPathResource("sql/scheme.sql");
        ClassPathResource data = new ClassPathResource("sql/data.sql");
        ResourceDatabasePopulator resourceDatabasePopulator = new ResourceDatabasePopulator(schema, data);

        ConnectionFactoryInitializer initializer = new ConnectionFactoryInitializer();
        initializer.setConnectionFactory(connectionFactory);
        initializer.setDatabasePopulator(resourceDatabasePopulator);
        return initializer;
    }
}
````

Si hasta este punto ejecutamos la aplicaci√≥n, veremos que el log nos muestra una ejecuci√≥n exitosa.

````bash
  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/

 :: Spring Boot ::                (v3.5.3)

2025-06-26T12:00:08.010-05:00  INFO 3520 --- [webflux-r2dbc-crud] [           main] d.m.r.app.WebfluxR2dbcCrudApplication    : Starting WebfluxR2dbcCrudApplication using Java 21.0.6 with PID 3520 (D:\programming\spring\02.youtube\10.joas_dev\webflux-r2dbc-crud\target\classes started by magadiflo in D:\programming\spring\02.youtube\10.joas_dev\webflux-r2dbc-crud)
2025-06-26T12:00:08.016-05:00  INFO 3520 --- [webflux-r2dbc-crud] [           main] d.m.r.app.WebfluxR2dbcCrudApplication    : No active profile set, falling back to 1 default profile: "default"
2025-06-26T12:00:09.038-05:00  INFO 3520 --- [webflux-r2dbc-crud] [           main] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data R2DBC repositories in DEFAULT mode.
2025-06-26T12:00:09.074-05:00  INFO 3520 --- [webflux-r2dbc-crud] [           main] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 18 ms. Found 0 R2DBC repository interfaces.
2025-06-26T12:00:10.703-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-1] io.r2dbc.postgresql.QUERY                : Executing query: SHOW TRANSACTION ISOLATION LEVEL
2025-06-26T12:00:10.714-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-1] io.r2dbc.postgresql.QUERY                : Executing query: SELECT oid, * FROM pg_catalog.pg_type WHERE typname IN ('hstore','geometry','vector')
2025-06-26T12:00:10.787-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [      Thread-16] io.r2dbc.postgresql.QUERY                : Executing query: DROP TABLE IF EXISTS book_authors
2025-06-26T12:00:10.796-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-2] io.r2dbc.postgresql.QUERY                : Executing query: SHOW TRANSACTION ISOLATION LEVEL
2025-06-26T12:00:10.798-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-2] io.r2dbc.postgresql.QUERY                : Executing query: SELECT oid, * FROM pg_catalog.pg_type WHERE typname IN ('hstore','geometry','vector')
2025-06-26T12:00:10.818-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-1] io.r2dbc.postgresql.QUERY                : Executing query: DROP TABLE IF EXISTS authors
2025-06-26T12:00:10.829-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-1] io.r2dbc.postgresql.QUERY                : Executing query: DROP TABLE IF EXISTS books
2025-06-26T12:00:10.836-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-1] io.r2dbc.postgresql.QUERY                : Executing query: CREATE TABLE authors( id SERIAL, first_name VARCHAR(45) NOT NULL, last_name VARCHAR(45) NOT NULL, birthdate DATE NOT NULL, CONSTRAINT pk_authors PRIMARY KEY(id) )
2025-06-26T12:00:10.860-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-3] io.r2dbc.postgresql.QUERY                : Executing query: SHOW TRANSACTION ISOLATION LEVEL
2025-06-26T12:00:10.861-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-3] io.r2dbc.postgresql.QUERY                : Executing query: SELECT oid, * FROM pg_catalog.pg_type WHERE typname IN ('hstore','geometry','vector')
2025-06-26T12:00:10.875-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-1] io.r2dbc.postgresql.QUERY                : Executing query: CREATE TABLE books( id SERIAL, title VARCHAR(255) NOT NULL, publication_date DATE NOT NULL, online_availability BOOLEAN DEFAULT FALSE, CONSTRAINT pk_books PRIMARY KEY(id) )
2025-06-26T12:00:10.882-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-1] io.r2dbc.postgresql.QUERY                : Executing query: CREATE TABLE book_authors( book_id INTEGER NOT NULL, author_id INTEGER NOT NULL, CONSTRAINT fk_books_book_authors FOREIGN KEY(book_id) REFERENCES books(id), CONSTRAINT fk_authors_book_authors FOREIGN KEY(author_id) REFERENCES authors(id), CONSTRAINT pk_book_authors PRIMARY KEY(book_id, author_id) )
2025-06-26T12:00:10.894-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [      Thread-17] io.r2dbc.postgresql.QUERY                : Executing query: INSERT INTO authors(first_name, last_name, birthdate) VALUES ('Milagros', 'D√≠az', '2006-06-15'), ('Lesly', '√Åguila', '1995-06-09'), ('Kiara', 'Lozano', '2001-10-03'), ('Briela', 'Cirilo', '1997-09-25')
2025-06-26T12:00:10.897-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-1] io.r2dbc.postgresql.QUERY                : Executing query: INSERT INTO books(title, publication_date, online_availability) VALUES ('Cien a√±os de soledad', '1999-01-15', true), ('Paco Yunque', '1985-03-18', true), ('Los perros hambrientos', '2002-05-06', false), ('Edipo Rey', '1988-07-15', true)
2025-06-26T12:00:10.900-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-1] io.r2dbc.postgresql.QUERY                : Executing query: INSERT INTO book_authors(book_id, author_id) VALUES (1, 1), (1, 2)
2025-06-26T12:00:10.903-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-1] io.r2dbc.postgresql.QUERY                : Executing query: INSERT INTO book_authors(book_id, author_id) VALUES (2, 3)
2025-06-26T12:00:10.948-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-4] io.r2dbc.postgresql.QUERY                : Executing query: SHOW TRANSACTION ISOLATION LEVEL
2025-06-26T12:00:10.950-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-4] io.r2dbc.postgresql.QUERY                : Executing query: SELECT oid, * FROM pg_catalog.pg_type WHERE typname IN ('hstore','geometry','vector')
2025-06-26T12:00:11.055-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-5] io.r2dbc.postgresql.QUERY                : Executing query: SHOW TRANSACTION ISOLATION LEVEL
2025-06-26T12:00:11.057-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-5] io.r2dbc.postgresql.QUERY                : Executing query: SELECT oid, * FROM pg_catalog.pg_type WHERE typname IN ('hstore','geometry','vector')
2025-06-26T12:00:11.128-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-6] io.r2dbc.postgresql.QUERY                : Executing query: SHOW TRANSACTION ISOLATION LEVEL
2025-06-26T12:00:11.130-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-6] io.r2dbc.postgresql.QUERY                : Executing query: SELECT oid, * FROM pg_catalog.pg_type WHERE typname IN ('hstore','geometry','vector')
2025-06-26T12:00:11.237-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-7] io.r2dbc.postgresql.QUERY                : Executing query: SHOW TRANSACTION ISOLATION LEVEL
2025-06-26T12:00:11.239-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-7] io.r2dbc.postgresql.QUERY                : Executing query: SELECT oid, * FROM pg_catalog.pg_type WHERE typname IN ('hstore','geometry','vector')
2025-06-26T12:00:11.343-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-8] io.r2dbc.postgresql.QUERY                : Executing query: SHOW TRANSACTION ISOLATION LEVEL
2025-06-26T12:00:11.344-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-8] io.r2dbc.postgresql.QUERY                : Executing query: SELECT oid, * FROM pg_catalog.pg_type WHERE typname IN ('hstore','geometry','vector')
2025-06-26T12:00:11.432-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-1] io.r2dbc.postgresql.QUERY                : Executing query: SHOW TRANSACTION ISOLATION LEVEL
2025-06-26T12:00:11.435-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-1] io.r2dbc.postgresql.QUERY                : Executing query: SELECT oid, * FROM pg_catalog.pg_type WHERE typname IN ('hstore','geometry','vector')
2025-06-26T12:00:11.546-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-2] io.r2dbc.postgresql.QUERY                : Executing query: SHOW TRANSACTION ISOLATION LEVEL
2025-06-26T12:00:11.548-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-2] io.r2dbc.postgresql.QUERY                : Executing query: SELECT oid, * FROM pg_catalog.pg_type WHERE typname IN ('hstore','geometry','vector')
2025-06-26T12:00:11.893-05:00  INFO 3520 --- [webflux-r2dbc-crud] [           main] o.s.b.web.embedded.netty.NettyWebServer  : Netty started on port 8080 (http)
2025-06-26T12:00:11.919-05:00  INFO 3520 --- [webflux-r2dbc-crud] [           main] d.m.r.app.WebfluxR2dbcCrudApplication    : Started WebfluxR2dbcCrudApplication in 4.713 seconds (process running for 6.126)
````

Y si ahora revisamos la base de datos `db_webflux_r2dbc` en postgres, veremos que las tablas se han creado
y poblado correctamente.

![02.png](assets/02.png)

## Creando entidades: Author, Book y BookAuthor

Creamos las entidades correspondientes a las tablas que definimos en el `scheme.sql`. Es importante recordar que aqu√≠
estamos trabajando con `R2DBC`, por lo tanto, hay que tener algunas consideraciones:

- No tenemos la anotaci√≥n `@Entity` en `R2DBC`.
- Las anotaciones `@Table` o `@Column` realmente no son necesarios, pero si necesitamos agregar alguna personalizaci√≥n
  podemos usarlos. Por ejemplo, la anotaci√≥n `@Table` nos permite redefinir el nombre de la tabla de la base de datos
  para cada entidad.
- La anotaci√≥n `@Id` es necesaria para identificar la clave primaria de la entidad.
- A diferencia de `JPA`, `R2DBC` no maneja autom√°ticamente las relaciones entre entidades (no hay `@OneToMany`,
  `@ManyToOne`, etc.).

````java

@ToString
@AllArgsConstructor
@NoArgsConstructor
@Builder
@Data
@Table(name = "authors")
public class Author {
    @Id
    private Integer id;
    private String firstName;
    private String lastName;
    private LocalDate birthdate;
}
````

````java

@ToString
@AllArgsConstructor
@NoArgsConstructor
@Builder
@Data
@Table(name = "books")
public class Book {
    @Id
    private Integer id;
    private String title;
    private LocalDate publicationDate;
    private Boolean onlineAvailability;
}
````

````java

@ToString
@AllArgsConstructor
@NoArgsConstructor
@Builder
@Data
@Table(name = "book_authors")
public class BookAuthor {
    private Integer bookId;
    private Integer authorId;
}
````

### ‚ö†Ô∏è Nota sobre Relaciones Muchos a Muchos en R2DBC

La tabla `book_authors` representa una **relaci√≥n muchos a muchos** entre `Author` y `Book`:

- Un autor puede escribir muchos libros
- Un libro puede ser escrito por muchos autores

### Dise√±o de Base de Datos

Para esta relaci√≥n `M:N`, hemos implementado una **clave primaria compuesta** que sigue el modelo relacional puro:

```sql
CREATE TABLE book_authors(
    book_id INTEGER NOT NULL,
    author_id INTEGER NOT NULL,
    CONSTRAINT fk_books_book_authors FOREIGN KEY(book_id) REFERENCES books(id),
    CONSTRAINT fk_authors_book_authors FOREIGN KEY(author_id) REFERENCES authors(id),
    CONSTRAINT pk_book_authors PRIMARY KEY(book_id, author_id) --Clave primaria compuesta
);
```

Este dise√±o:

- ‚úÖ `Garantiza unicidad`: No permite relaciones duplicadas.
- ‚úÖ `Es sem√°nticamente correcto`: La PK representa exactamente la relaci√≥n.
- ‚úÖ `Sigue el modelo relacional puro`: Sin campos artificiales innecesarios.

### Limitaci√≥n de R2DBC

`Spring Data R2DBC no soporta claves primarias compuestas` (a diferencia de JPA que tiene `@IdClass` y `@EmbeddedId`).
Por esta raz√≥n:

1. No podemos usar `ReactiveCrudRepository` para esta entidad.
2. Debemos usar `DatabaseClient` con `SQL nativo` para las operaciones `CRUD`.
3. Creamos una entidad `BookAuthor` `sin anotaci√≥n @Id` para representar la tabla.

Esta aproximaci√≥n nos permite mantener el dise√±o te√≥ricamente correcto en base de datos mientras trabajamos con las
limitaciones actuales de `R2DBC`.

### ‚ö†Ô∏è Importante

> Si quisi√©ramos utilizar `ReactiveCrudRepository` con la entidad `BookAuthor`, ser√≠a necesario agregar una columna
> adicional en la tabla `book_authors`, como un identificador √∫nico (por ejemplo, `id SERIAL PRIMARY KEY`).
> Esta nueva columna actuar√≠a como clave primaria y permitir√≠a anotar un atributo correspondiente en la entidad
> con `@Id`.
>
> Sin embargo, al introducir una clave artificial, la tabla dejar√≠a de representar un modelo relacional
> puramente compuesto, rompiendo en cierta forma con la normalizaci√≥n cl√°sica de una relaci√≥n muchos a muchos.
>
> Este caso lo podemos ver en el proyecto
> [webFlux-masterclass-microservices](https://github.com/magadiflo/webFlux-masterclass-microservices/tree/main/projects/webflux-playground/src/main/java/dev/magadiflo/app/sec03/entity)

## üß© Proyecci√≥n personalizada: `AuthorProjection`

`AuthorProjection` es una `proyecci√≥n basada en interfaz`, utilizada para representar parcialmente los datos de un
autor. `Spring Data` permite usar interfaces con solo los getters necesarios, lo que permite consultar solo los campos
deseados sin cargar toda la entidad.

> Para saber m√°s sobre proyecciones ir a este
> repositorio [spring-data-jpa-projections](https://github.com/magadiflo/spring-data-jpa-projections)

````java

@JsonPropertyOrder(value = {"firstName", "lastName", "fullName", "birthdate"})
public interface AuthorProjection {
    String getFirstName();

    String getLastName();

    LocalDate getBirthdate();

    default String getFullName() {
        if (Objects.isNull(getFirstName()) || Objects.isNull(getLastName())) {
            return "";
        }
        return "%s %s".formatted(getFirstName(), getLastName());
    }
}
````

Esta interfaz utiliza la anotaci√≥n `@JsonPropertyOrder(value = {"firstName", "lastName", "fullName", "birthdate"})`
para ordenar expl√≠citamente los campos en la respuesta JSON en el orden indicado.

Sin esta anotaci√≥n:

- Jackson no garantiza el orden de los campos.
- El orden puede cambiar entre ejecuciones, versiones de Spring, o versiones de Jackson.
- Campos default como fullName suelen aparecer en posiciones impredecibles.

Con esta anotaci√≥n:

- La respuesta JSON mantiene siempre un orden estable.
- Se mejora la claridad para clientes y frontends.
- Se facilita la validaci√≥n en tests automatizados.

### ‚ú® Nota importante

> La anotaci√≥n `@JsonPropertyOrder` solo es necesaria en `interfaces` de proyecci√≥n, ya que en estas el orden de las
> propiedades no est√° garantizado por defecto y puede variar.
>
> Si en lugar de una interfaz usamos un `record` o una `clase` DTO convencional, no necesitamos esta anotaci√≥n, ya que
> en un `record`, el orden de los campos en el JSON coincide con el orden en que se definen los componentes del record.
> En una clase, Jackson respeta el orden en que declares los atributos.

## üß© Proyecci√≥n personalizada: `BookProjection`

`BookProjection` es una proyecci√≥n personalizada utilizada para representar la vista combinada de un libro junto con
sus autores, generalmente como resultado de una consulta `SQL con JOIN` y `STRING_AGG`.

- El campo `authors` contiene una cadena con los nombres de los autores concatenados (por ejemplo: "`Alice Smith`,
  `Bob Johnson`"), generada por la base de datos.
- Este campo es interno y se marca con `@JsonIgnore` para que `no se exponga` directamente en la `respuesta JSON`.
- En su lugar, se expone el m√©todo `authorNames()`, anotado con `@JsonProperty`, que transforma esa cadena en una lista
  de nombres individuales (`List<String>`).

````java
public record BookProjection(String title,
                             LocalDate publicationDate,
                             Boolean onlineAvailability,
                             @JsonIgnore
                             String authors) {

    @JsonProperty
    public List<String> authorNames() {
        if (Objects.isNull(this.authors) || this.authors.isBlank()) {
            return List.of();
        }
        return Arrays.stream(this.authors.split(","))
                .map(String::trim)
                .toList();
    }
}
````

## Creando repositorios

A nuestras entidades `Author` y `Book` les crearemos a cada uno su interfaz de repositorio. Estos repositorios nos
permitir√°n interactuar con las tablas de la base de datos `authors` y `books`. Con respecto a la entidad `BookAuthor`,
esta la manejaremos dentro de una clase `dao` haciendo uso del `DatabaseClient`.

La interfaz `ReactiveCrudRepository` nos permitir√° usar sus m√©todos ya definidos, tales como el `save()`, `findById()`,
`findAll()`, `count()`, `delete()`, `deleteById()`, `deleteAll()`, etc.`

A continuaci√≥n se muestra la creaci√≥n del repositorio `BookRepository` para la entidad `Book`.

````java
public interface BookRepository extends ReactiveCrudRepository<Book, Integer> {
}
````

Otro repositorio que creamos es `AuthorRepository`. Al igual que otros repositorios en este proyecto, extiende de
`ReactiveCrudRepository`, lo que significa que ya incluye una serie de m√©todos predefinidos como `save()`, `findById()`,
`deleteById()`, entre otros.

> üß™ Sin embargo, en este caso particular decidimos definir nuestros propios m√©todos personalizados, con el objetivo de
> practicar distintas formas de interactuar con la base de datos de manera reactiva.
>
> Esto no solo nos permite comprender mejor c√≥mo funciona la capa de acceso a datos, sino que tambi√©n puede ser √∫til en
> escenarios donde se requiera mayor control sobre la consulta o comportamiento espec√≠fico.

````java
public interface AuthorRepository extends ReactiveCrudRepository<Author, Integer> {
    /**
     * @param author entity
     * @return affectedRows
     */
    @Modifying
    @Query(value = """
            INSERT INTO authors(first_name, last_name, birthdate)
            VALUES(:#{#author.firstName}, :#{#author.lastName}, :#{#author.birthdate})
            """)
    Mono<Integer> saveAuthor(@Param("author") Author author);

    /**
     * @param author entity
     * @return affectedRows
     */
    @Modifying
    @Query(value = """
            UPDATE authors
            SET first_name = :#{#author.firstName},
                last_name = :#{#author.lastName},
                birthdate = :#{#author.birthdate}
            WHERE id = :#{#author.id}
            """)
    Mono<Integer> updateAuthor(Author author);

    @Query("""
            SELECT a.id, a.first_name, a.last_name, a.birthdate
            FROM authors AS a
            WHERE a.id IN(:authorIds)
            """)
    Flux<Author> findAllAuthorsByIdIn(List<Integer> authorIds);

    @Query("""
            SELECT COUNT(a.id)
            FROM authors AS a
            WHERE a.first_name LIKE :#{'%' + #query + '%'}
                OR a.last_name LIKE :#{'%' + #query + '%'}
            """)
    Mono<Long> findCountByQuery(String query);

    @Query("""
            SELECT a.first_name, a.last_name, a.birthdate
            FROM authors AS a
            WHERE a.id = :authorId
            """)
    Mono<AuthorProjection> findAuthorById(Integer authorId);

    @Query("""
            SELECT a.first_name, a.last_name, a.birthdate
            FROM authors AS a
            WHERE a.first_name LIKE :#{'%' + #query + '%'}
                OR a.last_name LIKE :#{'%' + #query + '%'}
            ORDER BY a.id ASC
            LIMIT :#{#pageable.getPageSize()}
            OFFSET :#{#pageable.getOffset()}
            """)
    Flux<AuthorProjection> findByQuery(String query, Pageable pageable);

    @Modifying
    @Query("DELETE FROM authors AS a WHERE a.id = :authorId")
    Mono<Boolean> deleteAuthorById(Integer authorId);
}
````

En el repositorio `AuthorRepository` hemos definido m√©todos personalizados, donde:

- Usamos la anotaci√≥n `@Query()` para definir nuestra consulta.


- La consulta usada en la anotaci√≥n `@Query()` es `SQL nativo`, ya que estamos trabajando con `Spring Data R2DBC`
  y no con `Spring Data JPA`. Aunque dicho sea de paso, con `Spring Data JPA` tambi√©n se puede trabajar con `SQL nativo`
  solo que en ese caso es necesario agregar el atributo `nativeQuery` en la anotaci√≥n de la siguiente manera
  `@Query(value = "TU_CONSULTA_SQL_CON_JPA", nativeQuery = true)`, mientras que con `Spring Data R2DBC` usamos
  directamente `SQL nativo` en la anotaci√≥n `@Query`.


- La anotaci√≥n `@Modifying` indica que un m√©todo de consulta debe considerarse una consulta de modificaci√≥n que puede
  devolver:
    - `Void` para descartar el recuento de actualizaciones y esperar a que se complete.
    - `Integer` u otro tipo num√©rico que emite el recuento de filas afectadas.
    - `Boolean` para indicar si se actualiz√≥ al menos una fila.

  Los m√©todos de consulta anotados con `@Modifying` suelen ser instrucciones `INSERT, UPDATE, DELETE y DDL` que no
  devuelven resultados tabulares.


- Normalmente, cuando definimos par√°metros a nuestros m√©todos de repositorio, si son pocos par√°metros podemos definirlos
  uno a uno, pero si son muchos par√°metros, podemos pasarle directamente un objeto que tendr√° las propiedades que
  usaremos en la consulta. En nuestro caso, observemos la firma de nuestro m√©todo `saveAuthor()`
  `Mono<Integer> saveAuthor(@Param("author") Author author)`, le estamos pasando la clase `Author`.


- Para usar las propiedades del objeto pasado por par√°metro dentro de la consulta SQL usamos `SpEL`, por ejemplo:
  `:#{#author.firstName}`, donde `author` es el par√°metro definido en el m√©todo y `firstName` es la propiedad del
  objeto. Esta sintaxis se utiliza para acceder a expresiones `SpEL (Spring Expression Language)`. Permite referenciar
  propiedades y m√©todos de objetos directamente en la consulta.


- El prefijo `#{}` indica que se est√° utilizando `SpEL`, y el s√≠mbolo `#` se utiliza para acceder a los par√°metros del
  m√©todo o a las propiedades del objeto. Por ejemplo. `:#{#pageable.getPageSize()}` accede al m√©todo `getPageSize()` del
  objeto `Pageable` pasado como par√°metro.

Veamos un poco m√°s a detalle la siguiente consulta que hemos creado anteriormente.

````java

@Query("""
        SELECT a.first_name, a.last_name, a.birthdate
        FROM authors AS a
        WHERE a.first_name LIKE :#{'%' + #query + '%'}
            OR a.last_name LIKE :#{'%' + #query + '%'}
        ORDER BY a.id ASC
        LIMIT :#{#pageable.getPageSize()}
        OFFSET :#{#pageable.getOffset()}
        """)
Flux<AuthorProjection> findByQuery(String query, Pageable pageable);
````

Estamos pasando por par√°metro un `String query` y un `Pageable pageable`. Centr√©monos en el objeto `pageable`. Estamos
agregando este objeto `pageable` por par√°metro con la √∫nica finalidad de poder usar los valores internos que nos
proporcione su implementaci√≥n. En otras palabras, lo que pasamos por par√°metro al m√©todo `findByQuery(...)` es la
variable `query` y la implementaci√≥n de la interfaz `Pageable`. Esta implementaci√≥n la podemos obtener de un
`PageRequest.of(pageNumber, pageSize)`. Internamente, la implementaci√≥n hace ciertas operaciones, las mismas que
podemos obtenerlas, por ejemplo con el `getOffset()` que es la multiplicaci√≥n del `pageNumber * pageSize`.

Por otro lado, algo importante que se debe resaltar en las consultas personalizadas del repositorio anterior es que en
los m√©todos `findAuthorById` y `findByQuery` estamos usando el concepto de `Projections` (a modo de ejemplo), con
`projections` podemos recuperar del total de columnas que tenga una tabla, solo las columnas que queramos. Por ejemplo,
si nuestra tabla tuviera 50 columnas, con projections podemos recuperar solo 5 columnas, no todas, sino las que son
realmente necesarias.

## üßæ DTOs definidos con record

A continuaci√≥n, se presentan los DTOs creados utilizando la palabra clave `record`. Algunos de ellos incluyen m√©todos
adicionales que nos ayudar√°n m√°s adelante a evitar la duplicaci√≥n de c√≥digo y a mejorar la legibilidad en ciertas
operaciones l√≥gicas.

Iniciamos con el primer record llamado `BookCriteria` quien va a actuar como un `DTO (Data Transfer Object)` que
encapsula los posibles criterios para realizar b√∫squedas o filtros relacionados con libros. Es √∫til, por ejemplo, al
implementar endpoints de b√∫squeda din√°mica o filtros condicionales en un repositorio.

````java
public record BookCriteria(String query, LocalDate publicationDate) {
    public boolean hasQuery() {
        return Objects.nonNull(this.query) && !this.query.isBlank();
    }

    public boolean hasPublicationDate() {
        return Objects.nonNull(this.publicationDate);
    }
}
````

El siguiente `DTO (record)` llamado `BookRequest` representa la estructura de datos que se espera recibir al crear un
libro a trav√©s de una solicitud (por ejemplo, en un endpoint REST).

Incluye anotaciones de validaci√≥n para asegurar la integridad de los datos entrantes, as√≠ como un `constructor compacto`
que transforma el valor de un campo opcional y un m√©todo auxiliar que facilita la l√≥gica condicional.

````java
public record BookRequest(@NotBlank
                          @Size(min = 3)
                          String title,
                          @NotNull
                          LocalDate publicationDate,
                          Boolean onlineAvailability,
                          List<@NotNull Integer> authorIds) {
    // Constructor compacto
    public BookRequest {
        // El onlineAvailability es opcional. Si es null o false, ser√° falso. Caso contrario ser√° true
        // De esta manera se garantiza que siempre tenga un valor booleano definido y coherente
        onlineAvailability = Boolean.TRUE.equals(onlineAvailability);
    }

    public boolean hasNoAuthorIds() {
        return Objects.isNull(this.authorIds) || this.authorIds.isEmpty();
    }
}
````

‚úÖ Consideraciones

- Se aprovechan las validaciones con `Jakarta Bean Validation` directamente en los campos del record.
- `List<@NotNull Integer> authorIds`, lista de identificadores de autores relacionados con el libro.
  Cada elemento de la lista es validado con `@NotNull`, lo que impide que haya elementos `nulos` dentro de la colecci√≥n.
  Sin embargo, la lista en s√≠ puede ser `null` o `vac√≠a`, seg√∫n lo definido en la l√≥gica del m√©todo auxiliar
  `hasNoAuthorIds()`. Lo que no es v√°lido es que alg√∫n elemento individual dentro de la lista sea `null`.
- El constructor compacto permite aplicar l√≥gica limpia sin necesidad de crear una clase mutable.
- Se mantiene el c√≥digo conciso, inmutable y seguro para su uso en entornos reactivos.

El siguiente `DTO` `BookUpdateRequest` representa la estructura de datos necesaria para actualizar la informaci√≥n de
un libro existente.

A diferencia de otros `DTOs` de entrada como `BookRequest`, todos los campos en `BookUpdateRequest` son obligatorios,
lo cual asegura que la operaci√≥n de actualizaci√≥n sea completa y no parcial.

Se utilizan anotaciones de validaci√≥n para garantizar que:

- El t√≠tulo sea v√°lido y tenga una longitud m√≠nima.
- La fecha de publicaci√≥n y el estado de disponibilidad est√©n presentes.

Este de `DTO` ser√° utilizado en operaciones `PUT`.

````java
public record BookUpdateRequest(@NotBlank
                                @Size(min = 3)
                                String title,
                                @NotNull
                                LocalDate publicationDate,
                                @NotNull
                                Boolean onlineAvailability) {
}
````

El siguiente `DTO` `BookAuthorUpdateRequest` representa la estructura esperada para actualizar la lista de autores
asociados a un libro.

Contiene un √∫nico campo: una lista de identificadores de autores (`authorIds`), en la cual cada elemento debe ser no
nulo, gracias a la anotaci√≥n `@NotNull` aplicada sobre los elementos de la lista.

> ‚ö†Ô∏è Importante:
>
> La lista en s√≠ (`authorIds`) puede ser `null` o `vac√≠a` si as√≠ lo permite la l√≥gica de negocio. Sin embargo, ning√∫n
> elemento individual de la lista puede ser `null`, lo cual garantiza que los valores enviados sean identificadores
> v√°lidos.

Este `DTO` es √∫til cuando se quiere actualizar solo los autores relacionados con un libro, manteniendo separados los
cambios estructurales (como t√≠tulo o fecha) de los cambios relacionales.

````java
public record BookAuthorUpdateRequest(List<@NotNull Integer> authorIds) {
}
````

A continuaci√≥n se mostrar√°n otros `DTOs` creados para la aplicaci√≥n.

````java
public record BookResponse(Integer id,
                           String title,
                           LocalDate publicationDate,
                           Boolean onlineAvailability) {
}
````

````java
public record AuthorRequest(String firstName,
                            String lastName,
                            LocalDate birthdate) {
}
````

````java
public record AuthorResponse(Integer id,
                             String firstName,
                             String lastName,
                             LocalDate birthdate) {
}
````

````java
public record AuthorCriteria(String firstName, String lastName) {
}
````

## üõ†Ô∏è DAO personalizado con `R2dbcEntityTemplate`: `AuthorDaoImpl`

Este `DAO` implementa una l√≥gica de consulta din√°mica para la entidad `Author`, utilizando `R2dbcEntityTemplate`,
una alternativa program√°tica y orientada a objetos a las consultas `SQL manuales`.

> Su uso es comparable al `EntityManager` en `JPA`, pero en el contexto de `Spring Data R2DBC` y programaci√≥n reactiva.
> Es decir en `JPA` utilizamos `EntityManager`
> [(spring-data-jpa-criteria-queries)](https://github.com/magadiflo/spring-data-jpa-criteria-queries/blob/main/src/main/java/dev/magadiflo/criteria_queries/persistence/dao/EmployeeSearchDao.java)
> mientras que aqu√≠ en `Spring Data R2DBC` y programaci√≥n reactiva usamos `R2dbcEntityTemplate`.

````java
public interface AuthorDao {
    Flux<Author> findAuthorByCriteria(AuthorCriteria authorCriteria);
}
````

Define una operaci√≥n de b√∫squeda que recibe un objeto `AuthorCriteria` con los posibles filtros (nombre y apellido del
autor).

````java
import org.springframework.data.relational.core.query.Criteria;

@RequiredArgsConstructor
@Repository
public class AuthorDaoImpl implements AuthorDao {

    private final R2dbcEntityTemplate template;

    @Override
    public Flux<Author> findAuthorByCriteria(AuthorCriteria authorCriteria) {
        Criteria criteria = Criteria.empty();

        if (Objects.nonNull(authorCriteria.firstName()) && !authorCriteria.firstName().trim().isEmpty()) {
            Criteria likeFirstName = Criteria.where("first_name").like(this.likePattern(authorCriteria.firstName()));
            criteria = criteria.and(likeFirstName);
        }

        if (Objects.nonNull(authorCriteria.lastName()) && !authorCriteria.lastName().trim().isEmpty()) {
            Criteria likeLastName = Criteria.where("last_name").like(this.likePattern(authorCriteria.lastName()));
            criteria = criteria.and(likeLastName);
        }

        Query query = Query.query(criteria);

        return this.template
                .select(Author.class)
                .matching(query)
                .all();
    }

    private String likePattern(String value) {
        return "%" + value.trim() + "%";
    }
}
````

### ‚öôÔ∏è ¬øQu√© hace esta clase?

- Usa `R2dbcEntityTemplate` para construir y ejecutar consultas reactivas sin necesidad de escribir SQL.
- La l√≥gica de filtrado es din√°mica:
    - Si el `firstName` o `lastName` est√° presente en el `AuthorCriteria`, se aplica una cl√°usula `LIKE`.
    - Los filtros se combinan mediante `Criteria` (una clase propia de `Spring Data R2DBC`).
- Finalmente, se ejecuta la consulta con `.select(...).matching(query).all()` y se retorna un `Flux<Author>`.

### üí° Ventajas del uso de `R2dbcEntityTemplate`

- Permite construir consultas m√°s expresivas y legibles usando clases en lugar de strings SQL.
- Evita errores comunes de SQL al construir las queries de forma program√°tica.
- Ideal para escenarios donde se requiere construir filtros condicionales y consultas din√°micas.
- Conserva el enfoque reactivo de extremo a extremo.

### ‚ö†Ô∏è Limitaciones de `R2dbcEntityTemplate`

Aunque `R2dbcEntityTemplate` facilita la construcci√≥n de consultas reactivas de manera program√°tica, tiene algunas
limitaciones importantes:

- `Solo opera sobre una √∫nica entidad a la vez`: No permite realizar `joins` entre tablas directamente como lo har√≠as
  con `SQL nativo` o con `DatabaseClient`. Esto significa que si necesitas combinar datos de varias entidades
  (por ejemplo, `Author` con `Book`), deber√°s:
    - Hacer m√∫ltiples consultas separadas.
    - O recurrir a `DatabaseClient` y construir una consulta SQL manual.


- `Falta de soporte para expresiones m√°s complejas`: Las operaciones avanzadas como `subconsultas`, `GROUP BY`,
  `HAVING`, `funciones agregadas`, etc., est√°n fuera del alcance del `API de Criteria`.

## üõ†Ô∏è DAO personalizado con `DatabaseClient`: `BookAuthorDaoImpl`

Este DAO implementa m√∫ltiples operaciones personalizadas para gestionar la relaci√≥n entre `libros` y `autores`
(`book_authors`) utilizando `DatabaseClient`, un componente de bajo nivel en `Spring Data R2DBC` que permite ejecutar
consultas `SQL nativas` de forma reactiva.

> Podemos considerar a `DatabaseClient` como el equivalente reactivo de `JdbcTemplate`
> [(spring-data-jdbc-template-crud-api-rest):](https://github.com/magadiflo/spring-data-jdbc-template-crud-api-rest/blob/main/src/main/java/com/magadiflo/jdbc/template/app/repository/impl/UserRepositoryImpl.java)
> - En ambos casos se utiliza SQL nativo directamente para construir y ejecutar consultas.
> - Tienes control total sobre la estructura de la consulta (SELECT, JOIN, GROUP BY, etc.).
> - Los par√°metros se enlazan de forma segura (:paramName en DatabaseClient, ? o :param en JdbcTemplate).
> - Eres responsable de mapear manualmente los resultados (`RowMapper en JdbcTemplate`,
    `Row + RowMetadata en DatabaseClient`).

````java
public interface BookAuthorDao {
    Mono<Long> countBookAuthorByCriteria(BookCriteria bookCriteria);

    Mono<Long> saveBookAuthor(BookAuthor bookAuthor);

    Mono<Void> saveAllBookAuthor(List<BookAuthor> bookAuthorList);

    Mono<Boolean> existBookAuthorByBookId(Integer bookId);

    Mono<Boolean> existBookAuthorByAuthorId(Integer authorId);

    Mono<Void> deleteBookAuthorByBookId(Integer bookId);

    Mono<Void> deleteBookAuthorByAuthorId(Integer authorId);

    Mono<BookProjection> findBookWithTheirAuthorsByBookId(Integer bookId);

    Flux<BookProjection> findAllToPage(BookCriteria bookCriteria, Pageable pageable);
}
````

### ‚öôÔ∏è ¬øQu√© es DatabaseClient y por qu√© se usa?

`DatabaseClient` proporciona una interfaz program√°tica, flexible y orientada a SQL para ejecutar consultas directamente
sobre la base de datos. Se utiliza en este DAO porque:

- Permite escribir consultas SQL completamente personalizadas, incluyendo:
    - Joins entre m√∫ltiples tablas.
    - Uso de funciones agregadas como STRING_AGG.
    - Subconsultas o estructuras complejas.
    - Paginaci√≥n (LIMIT, OFFSET).

- Soporta binding de par√°metros nombrados (`:param`) para prevenir inyecciones SQL y facilitar la reutilizaci√≥n.
- Ofrece control total sobre el resultado: puedes mapear manualmente filas (`Row`) a cualquier tipo de objeto (como
  `BookProjection`).
- Es ideal para escenarios donde `R2dbcEntityTemplate` se queda corto, como cuando se necesita trabajar con m√∫ltiples
  entidades o resultados no directamente relacionados con clases del modelo.

````java

@Slf4j
@RequiredArgsConstructor
@Repository
public class BookAuthorDaoImpl implements BookAuthorDao {

    private final DatabaseClient databaseClient;
    private static final String BOOK_ID = "bookId";
    private static final String AUTHOR_ID = "authorId";

    @Override
    public Mono<Long> countBookAuthorByCriteria(BookCriteria bookCriteria) {
        String sql = this.buildCountSql(bookCriteria);
        DatabaseClient.GenericExecuteSpec querySpec = this.databaseClient.sql(sql);
        querySpec = this.bindCriteriaParameters(querySpec, bookCriteria);
        return querySpec
                .map((row, rowMetadata) -> row.get("total", Long.class))
                .one();
    }

    @Override
    public Mono<Long> saveBookAuthor(BookAuthor bookAuthor) {
        return this.rowsUpdatedAfterInsert(bookAuthor);
    }

    @Override
    public Mono<Void> saveAllBookAuthor(List<BookAuthor> bookAuthorList) {
        return Flux.fromIterable(bookAuthorList)
                .flatMap(this::rowsUpdatedAfterInsert)
                .then();
    }

    @Override
    public Mono<Boolean> existBookAuthorByBookId(Integer bookId) {
        String sql = """
                SELECT EXISTS(
                    SELECT 1
                    FROM book_authors AS ba
                    WHERE ba.book_id = :bookId
                ) AS result
                """;
        return this.databaseClient
                .sql(sql)
                .bind(BOOK_ID, bookId)
                .map((row, rowMetadata) -> row.get("result", Boolean.class))
                .one();
    }

    @Override
    public Mono<Boolean> existBookAuthorByAuthorId(Integer authorId) {
        String sql = """
                SELECT EXISTS(
                    SELECT 1
                    FROM book_authors AS ba
                    WHERE ba.author_id = :authorId
                ) AS result
                """;
        return this.databaseClient
                .sql(sql)
                .bind(AUTHOR_ID, authorId)
                .map((row, rowMetadata) -> row.get("result", Boolean.class))
                .one();
    }

    @Override
    public Mono<Void> deleteBookAuthorByBookId(Integer bookId) {
        String sql = """
                DELETE FROM book_authors
                WHERE book_id = :bookId
                """;
        return this.databaseClient
                .sql(sql)
                .bind(BOOK_ID, bookId)
                .then();
    }

    @Override
    public Mono<Void> deleteBookAuthorByAuthorId(Integer authorId) {
        String sql = """
                DELETE FROM book_authors
                WHERE author_id = :authorId
                """;
        return this.databaseClient
                .sql(sql)
                .bind(AUTHOR_ID, authorId)
                .then();
    }

    @Override
    public Mono<BookProjection> findBookWithTheirAuthorsByBookId(Integer bookId) {
        String sql = """
                SELECT b.title,
                        b.publication_date,
                        b.online_availability,
                        STRING_AGG(a.first_name||' '||a.last_name, ', ') as concat_authors
                FROM books AS b
                    LEFT JOIN book_authors AS ba ON(b.id = ba.book_id)
                    LEFT JOIN authors AS a ON(ba.author_id = a.id)
                WHERE b.id = :bookId
                GROUP BY b.title,
                        b.publication_date,
                        b.online_availability
                """;
        return this.databaseClient
                .sql(sql)
                .bind(BOOK_ID, bookId)
                .map(this.mappingBookProjection())
                .one();
    }

    @Override
    public Flux<BookProjection> findAllToPage(BookCriteria bookCriteria, Pageable pageable) {
        String sql = this.buildDetailSql(bookCriteria);
        DatabaseClient.GenericExecuteSpec querySpec = this.databaseClient.sql(sql);
        querySpec = this.bindCriteriaParameters(querySpec, bookCriteria);
        querySpec = querySpec
                .bind("limit", pageable.getPageSize())
                .bind("offset", pageable.getOffset());
        return querySpec
                .map(this.mappingBookProjection())
                .all();
    }

    private BiFunction<Row, RowMetadata, BookProjection> mappingBookProjection() {
        return (row, rowMetadata) -> new BookProjection(
                row.get("title", String.class),
                row.get("publication_date", LocalDate.class),
                row.get("online_availability", Boolean.class),
                row.get("concat_authors", String.class)
        );
    }

    private Mono<Long> rowsUpdatedAfterInsert(BookAuthor bookAuthor) {
        String sql = """
                INSERT INTO book_authors(book_id, author_id)
                VALUES(:bookId, :authorId)
                """;
        return this.databaseClient
                .sql(sql)
                .bind(BOOK_ID, bookAuthor.getBookId())
                .bind(AUTHOR_ID, bookAuthor.getAuthorId())
                .fetch()
                .rowsUpdated();
    }

    private String buildDetailSql(BookCriteria bookCriteria) {
        StringBuilder sql = new StringBuilder("""
                SELECT b.title,
                        b.publication_date,
                        b.online_availability,
                        STRING_AGG(a.first_name||' '||a.last_name, ', ') as concat_authors
                FROM books AS b
                    LEFT JOIN book_authors AS ba ON(b.id = ba.book_id)
                    LEFT JOIN authors AS a ON(ba.author_id = a.id)
                """);
        sql.append(this.buildWhereClause(bookCriteria));
        sql.append("""
                GROUP BY b.title,
                        b.publication_date,
                        b.online_availability
                ORDER BY b.title
                LIMIT :limit
                OFFSET :offset
                """);
        return sql.toString();
    }

    private String buildCountSql(BookCriteria bookCriteria) {
        StringBuilder sql = new StringBuilder("""
                SELECT COUNT(*) AS total
                FROM (
                    SELECT b.id
                    FROM books AS b
                    LEFT JOIN book_authors AS ba ON (b.id = ba.book_id)
                    LEFT JOIN authors AS a ON (ba.author_id = a.id)
                """);
        sql.append(this.buildWhereClause(bookCriteria));
        sql.append("""
                    GROUP BY b.id
                ) AS unique_books
                """);
        return sql.toString();
    }

    private String buildWhereClause(BookCriteria bookCriteria) {
        List<String> conditions = new ArrayList<>();

        if (bookCriteria.hasQuery()) {
            conditions.add("""
                    (
                        LOWER(b.title) LIKE LOWER(:query)
                        OR LOWER(a.first_name) LIKE LOWER(:query)
                        OR LOWER(a.last_name) LIKE LOWER(:query)
                    )
                    """);
        }

        if (bookCriteria.hasPublicationDate()) {
            conditions.add("b.publication_date = :publicationDate");
        }

        if (conditions.isEmpty()) {
            return "";
        }

        return "WHERE %s%n".formatted(String.join(" AND ", conditions));
    }

    private DatabaseClient.GenericExecuteSpec bindCriteriaParameters(DatabaseClient.GenericExecuteSpec spec, BookCriteria bookCriteria) {
        if (bookCriteria.hasQuery()) {
            String likePattern = "%" + bookCriteria.query().trim() + "%";
            spec = spec.bind("query", likePattern);
        }

        if (bookCriteria.hasPublicationDate()) {
            spec = spec.bind("publicationDate", bookCriteria.publicationDate());
        }
        return spec;
    }
}
````

## ‚öñÔ∏è Comparaci√≥n: `R2dbcEntityTemplate` vs. `DatabaseClient`

| Caracter√≠stica               | `R2dbcEntityTemplate`                    | `DatabaseClient`                                 |
|------------------------------|------------------------------------------|--------------------------------------------------|
| üîç Nivel de abstracci√≥n      | Alto (orientado a entidades)             | Bajo (SQL nativo)                                |
| üß± Tipo de consulta          | Program√°tica (Criteria API)              | SQL manual                                       |
| ü§ù Soporte para joins        | ‚ùå No                                     | ‚úÖ S√≠                                             |
| üéØ Uso t√≠pico                | Consultas simples sobre una sola entidad | Consultas complejas, joins, vistas, proyecciones |
| üßë‚Äçüíª Control sobre SQL      | Limitado                                 | Total                                            |
| üß© S√≠mil en stack bloqueante | `EntityManager` (JPA)                    | `JdbcTemplate`                                   |

### üìù ¬øCu√°ndo usar cada uno?

- Usa `R2dbcEntityTemplate` cuando necesitas construir consultas reactivas simples, con filtros din√°micos sobre una sola
  entidad, sin necesidad de escribir SQL directamente.
- Usa `DatabaseClient` cuando necesitas mayor flexibilidad, trabajar con m√∫ltiples tablas, funciones agregadas,
  paginaci√≥n avanzada o SQL personalizado.

## üîÑ Mapper con MapStruct

Creamos la interfaz `BookMapper` para realizar la conversi√≥n (`mapping`) entre diferentes tipos de objetos
relacionados con la entidad `Book`. Utiliza `MapStruct`, un generador de c√≥digo de mapeo que crea implementaciones
en tiempo de compilaci√≥n, evitando as√≠ la necesidad de escribir conversiones manuales.

Este mapper en particular realiza:

- Conversi√≥n de entidad `Book` a `DTO` de respuesta (`BookResponse`).
- Conversi√≥n de `DTO` de entrada (`BookRequest`) a entidad `Book`.
- Actualizaci√≥n parcial de una entidad `Book` a partir de un `DTO` de actualizaci√≥n (`BookUpdateRequest`), ignorando el
  campo id.

### üè∑Ô∏è Anotaciones utilizadas

`@Mapper(componentModel = MappingConstants.ComponentModel.SPRING)`

- Indica que esta interfaz es un mapper de `MapStruct`.
- `componentModel = "spring"` permite que la implementaci√≥n generada sea un `bean` de `Spring`, de modo que pueda ser
  inyectada con `@Autowired` o `@RequiredArgsConstructor`.

`@Mapping(target = "id", ignore = true)`

- Se aplica al m√©todo `toBookUpdate`.
- Indica que el campo `id` de la entidad `Book` no debe ser sobrescrito durante el mapeo desde el `DTO`.
- Es √∫til cuando actualizamos una entidad existente, pero queremos conservar su identificador.

`@MappingTarget`

- Apunta al par√°metro que se va a modificar directamente durante el mapeo.
- En este caso, permite que book sea actualizado en lugar de crear un nuevo objeto.
- Es ideal para operaciones `PUT` o `PATCH` donde actualizas una entidad persistente.

````java

@Mapper(componentModel = MappingConstants.ComponentModel.SPRING)
public interface BookMapper {
    BookResponse toBookResponse(Book book);

    Book toBook(BookRequest bookRequest);

    @Mapping(target = "id", ignore = true)
    Book toBookUpdate(@MappingTarget Book book, BookUpdateRequest bookUpdateRequest);
}
````

Para el mapeo de la entidad Author con su dto tambi√©n definimos mapeador (`AuthorMapper`).

````java

@Mapper(componentModel = MappingConstants.ComponentModel.SPRING)
public interface AuthorMapper {
    AuthorResponse toAuthorResponse(Author author);

    Author toAuthor(AuthorRequest authorRequest);
}
````

## ‚ùå Manejo centralizado de errores: `ApplicationExceptions`

`ApplicationExceptions` es una clase utilitaria dise√±ada para centralizar la construcci√≥n de errores reactivos
(`Mono.error`) dentro de la aplicaci√≥n. Esto permite lanzar excepciones personalizadas de forma consistente y
reutilizable desde cualquier componente reactivo (servicios, DAOs, validaciones, etc.).

`@NoArgsConstructor(access = AccessLevel.PRIVATE)`

- Genera un constructor privado sin argumentos.
- Esto impide que se creen instancias de la clase, forzando su uso est√°tico.
- Refuerza la intenci√≥n de que esta clase sea solo utilitaria.

````java

@NoArgsConstructor(access = AccessLevel.PRIVATE)
public class ApplicationExceptions {

    public static <T> Mono<T> authorNotFound(Integer authorId) {
        return Mono.error(() -> new AuthorNotFoundException(authorId));
    }

    public static <T> Mono<T> bookNotFound(Integer bookId) {
        return Mono.error(() -> new BookNotFoundException(bookId));
    }

    public static <T> Mono<T> authorIdsNotFound() {
        return Mono.error(AuthorIdsNotFoundException::new);
    }

    public static <T> Mono<T> missingFirstName() {
        return Mono.error(() -> new InvalidInputException("El nombre es requerido"));
    }

    public static <T> Mono<T> missingLastName() {
        return Mono.error(() -> new InvalidInputException("El apellido es requerido"));
    }

    public static <T> Mono<T> missingBirthdate() {
        return Mono.error(() -> new InvalidInputException("La fecha de nacimiento es requerido"));
    }
}
````

A continuaci√≥n se muestran las distintas clases de excepci√≥n que utiliza la clase `ApplicationExceptions`.

````java
public class AuthorNotFoundException extends RuntimeException {

    private static final String MESSAGE = "El author [id=%d] no fue encontrado";

    public AuthorNotFoundException(Integer authorId) {
        super(MESSAGE.formatted(authorId));
    }
}
````

````java
public class BookNotFoundException extends RuntimeException {

    private static final String MESSAGE = "El libro [id=%d] no fue encontrado";

    public BookNotFoundException(Integer authorId) {
        super(MESSAGE.formatted(authorId));
    }
}
````

````java
public class AuthorIdsNotFoundException extends RuntimeException {
    public AuthorIdsNotFoundException() {
        super("Algunos IDs de autores no existen en el sistema");
    }
}
````

````java
public class InvalidInputException extends RuntimeException {
    public InvalidInputException(String message) {
        super(message);
    }
}
````

## üß± Manejo global de excepciones HTTP: `ApplicationExceptionHandler`

`ApplicationExceptionHandler` es una clase anotada con `@RestControllerAdvice` que act√∫a como manejador global de
excepciones para toda la aplicaci√≥n `WebFlux`.

Su responsabilidad principal es interceptar excepciones personalizadas o comunes, y transformarlas en respuestas HTTP
con estructura clara y uniforme, usando el tipo `ProblemDetail`, definido en `spring-web`.

### üß© ¬øQu√© hace?

- Captura excepciones lanzadas desde controladores y servicios (como `AuthorNotFoundException`, `InvalidInputException`,
  etc.).
- Crea una instancia de `ProblemDetail` que representa un error HTTP estructurado.
- Devuelve la excepci√≥n traducida como `ResponseEntity<ProblemDetail>` con el c√≥digo de estado correspondiente (`404`,
  `400`, `500`, etc.).
- A√±ade detalles adicionales como:
    - `title`: resumen del error.
    - `detail`: mensaje t√©cnico de la excepci√≥n.
    - Propiedades adicionales como `errors` en caso de validaci√≥n.

````java

@Slf4j
@RestControllerAdvice
public class ApplicationExceptionHandler {

    @ExceptionHandler(AuthorNotFoundException.class)
    public Mono<ResponseEntity<ProblemDetail>> handleException(AuthorNotFoundException exception) {
        ProblemDetail problemDetail = this.build(HttpStatus.NOT_FOUND, exception, detail -> {
            detail.setTitle("Autor no encontrado");
        });
        return Mono.just(ResponseEntity.status(HttpStatus.NOT_FOUND).body(problemDetail));
    }

    @ExceptionHandler(InvalidInputException.class)
    public Mono<ResponseEntity<ProblemDetail>> handleException(InvalidInputException exception) {
        ProblemDetail problemDetail = this.build(HttpStatus.BAD_REQUEST, exception, detail -> {
            detail.setTitle("Entrada no v√°lida");
        });
        return Mono.just(ResponseEntity.status(HttpStatus.BAD_REQUEST).body(problemDetail));
    }

    @ExceptionHandler(ServerWebInputException.class)
    public Mono<ResponseEntity<ProblemDetail>> handleDecodingException(ServerWebInputException exception) {
        ProblemDetail problemDetail = this.build(HttpStatus.BAD_REQUEST, exception, detail -> {
            detail.setTitle("Error de formato de la petici√≥n");
            log.info("{}", exception.getMostSpecificCause().getMessage());
        });
        return Mono.just(ResponseEntity.status(HttpStatus.BAD_REQUEST).body(problemDetail));
    }

    @ExceptionHandler(AuthorIdsNotFoundException.class)
    public Mono<ResponseEntity<ProblemDetail>> handleException(AuthorIdsNotFoundException exception) {
        ProblemDetail problemDetail = this.build(HttpStatus.NOT_FOUND, exception, detail -> {
            detail.setTitle("Autor no encontrado");
        });
        return Mono.just(ResponseEntity.status(HttpStatus.NOT_FOUND).body(problemDetail));
    }

    @ExceptionHandler(BookNotFoundException.class)
    public Mono<ResponseEntity<ProblemDetail>> handleException(BookNotFoundException exception) {
        ProblemDetail problemDetail = this.build(HttpStatus.NOT_FOUND, exception, detail -> {
            detail.setTitle("Libro no encontrado");
        });
        return Mono.just(ResponseEntity.status(HttpStatus.NOT_FOUND).body(problemDetail));
    }

    @ExceptionHandler(WebExchangeBindException.class)
    public Mono<ResponseEntity<ProblemDetail>> handleException(WebExchangeBindException exception) {
        Map<String, List<String>> errors = exception.getBindingResult().getFieldErrors().stream()
                .collect(Collectors.groupingBy(
                        FieldError::getField,
                        Collectors.mapping(
                                DefaultMessageSourceResolvable::getDefaultMessage,
                                Collectors.toList()
                        )
                ));
        ProblemDetail problemDetail = this.build(HttpStatus.BAD_REQUEST, exception, detail -> {
            detail.setTitle("El cuerpo de la petici√≥n contiene valores no v√°lidos");
            detail.setDetail(exception.getReason());
            detail.setProperty("errors", errors);
        });
        return Mono.just(ResponseEntity.status(HttpStatus.BAD_REQUEST).body(problemDetail));
    }

    @ExceptionHandler(Exception.class)
    public Mono<ResponseEntity<ProblemDetail>> handleException(Exception exception) {
        ProblemDetail problemDetail = this.build(HttpStatus.INTERNAL_SERVER_ERROR, exception, detail -> {
            detail.setTitle("Se produjo un error interno en el servidor");
        });
        return Mono.just(ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(problemDetail));
    }

    private ProblemDetail build(HttpStatus status, Exception exception, Consumer<ProblemDetail> detailConsumer) {
        log.info("{}", exception.getMessage());
        ProblemDetail problemDetail = ProblemDetail.forStatusAndDetail(status, exception.getMessage());
        detailConsumer.accept(problemDetail);
        return problemDetail;
    }
}
````

## üß© Servicio de dominio: `AuthorService`

La interfaz `AuthorService` define las operaciones principales del dominio `Author`, desacoplando la l√≥gica de negocio
de su implementaci√≥n. Proporciona soporte para:

- Listado total de autores.
- B√∫squeda por ID con proyecci√≥n.
- Paginaci√≥n con filtros.
- Creaci√≥n, actualizaci√≥n y eliminaci√≥n de autores.

````java
public interface AuthorService {
    Flux<AuthorResponse> findAllAuthors();

    Mono<AuthorProjection> findAuthorById(Integer authorId);

    Mono<Page<AuthorProjection>> getAllAuthorsToPage(String query, int pageNumber, int pageSize);

    Mono<Integer> saveAuthor(AuthorRequest authorRequest);

    Mono<AuthorProjection> updateAuthor(Integer authorId, AuthorRequest authorRequest);

    Mono<Boolean> deleteAuthor(Integer authorId);
}
````

La clase `AuthorServiceImpl` contiene la l√≥gica de negocio reactiva para trabajar con autores, apoy√°ndose en
componentes como:

- `AuthorRepository`: acceso principal a la base de datos.
- `BookAuthorDao`: verificaci√≥n y eliminaci√≥n de relaciones entre libros y autores.
- `AuthorMapper`: conversi√≥n entre entidades y DTOs.

### üîç Comportamientos destacados

- `Lectura reactiva y eficiente`: m√©todos como `findAllAuthors()` o `getAllAuthorsToPage()` devuelven flujos
  (`Flux` / `Mono`) desde repositorios, manteniendo la naturaleza no bloqueante de `WebFlux`.
- `Paginaci√≥n con conteo total`: se utiliza `Mono.zip(...)` para `obtener simult√°neamente` la lista paginada y el total
  de resultados, y luego se construye un objeto `PageImpl`.
- `Validaci√≥n de existencia`: se centraliza la validaci√≥n de existencia con
  `switchIfEmpty(ApplicationExceptions.authorNotFound(...))`, asegurando respuestas consistentes.
- `Mapeo de DTOs`: se usa `AuthorMapper` para convertir entre `AuthorRequest` y `Author`, separando claramente las
  capas.
- `Eliminaci√≥n con relaci√≥n`: al eliminar un autor, se verifica si tiene libros relacionados. Si los tiene, se eliminan
  primero de la tabla intermedia (`book_authors`) antes de eliminar el autor.

````java

@Slf4j
@RequiredArgsConstructor
@Service
public class AuthorServiceImpl implements AuthorService {

    private final AuthorRepository authorRepository;
    private final BookAuthorDao bookAuthorDao;
    private final AuthorMapper authorMapper;

    @Override
    @Transactional(readOnly = true)
    public Flux<AuthorResponse> findAllAuthors() {
        return this.authorRepository.findAll()
                .map(this.authorMapper::toAuthorResponse);
    }

    @Override
    @Transactional(readOnly = true)
    public Mono<AuthorProjection> findAuthorById(Integer authorId) {
        return this.authorRepository.findAuthorById(authorId)
                .switchIfEmpty(ApplicationExceptions.authorNotFound(authorId));
    }

    @Override
    @Transactional(readOnly = true)
    public Mono<Page<AuthorProjection>> getAllAuthorsToPage(String query, int pageNumber, int pageSize) {
        Pageable pageable = PageRequest.of(pageNumber, pageSize);
        return Mono.zip(
                this.authorRepository.findByQuery(query, pageable).collectList(),
                this.authorRepository.findCountByQuery(query),
                (data, total) -> new PageImpl<>(data, pageable, total)
        );
    }

    @Override
    @Transactional
    public Mono<Integer> saveAuthor(AuthorRequest authorRequest) {
        return Mono.fromSupplier(() -> this.authorMapper.toAuthor(authorRequest))
                .flatMap(this.authorRepository::saveAuthor);
    }

    @Override
    @Transactional
    public Mono<AuthorProjection> updateAuthor(Integer authorId, AuthorRequest authorRequest) {
        return this.authorRepository.findById(authorId)
                .map(author -> this.authorMapper.toAuthor(authorRequest))
                .doOnNext(author -> author.setId(authorId))
                .flatMap(this.authorRepository::updateAuthor)
                .flatMap(affectedRows -> this.authorRepository.findAuthorById(authorId))
                .switchIfEmpty(ApplicationExceptions.authorNotFound(authorId));
    }

    @Override
    @Transactional
    public Mono<Boolean> deleteAuthor(Integer authorId) {
        return this.authorRepository.findById(authorId)
                .switchIfEmpty(ApplicationExceptions.authorNotFound(authorId))
                .flatMap(author -> this.bookAuthorDao.existBookAuthorByAuthorId(authorId))
                .flatMap(hasBooks -> Boolean.TRUE.equals(hasBooks) ? this.bookAuthorDao.deleteBookAuthorByAuthorId(authorId) : Mono.empty())
                .then(this.authorRepository.deleteAuthorById(authorId));
    }
}
````

## üìÑ Anotaci√≥n `@Transactional` en aplicaciones reactivas con Spring WebFlux y R2DBC

### üéØ Objetivo General

Verificar y documentar el uso de la anotaci√≥n `@Transactional` en aplicaciones reactivas, espec√≠ficamente en un
proyecto con:

- Spring WebFlux
- Spring Data R2DBC
- PostgreSQL

Se sabe de antemano que:

> ‚úÖ La anotaci√≥n `@Transactional` funciona correctamente en aplicaciones reactivas para delimitar transacciones que
> modifican la base de datos (`INSERT`, `UPDATE`, `DELETE`).

‚ö†Ô∏è El enfoque de estas pruebas fue comprobar espec√≠ficamente que `@Transactional(readOnly = true)` efectivamente
bloquea escrituras.

Para ello se dise√±aron experimentos que forzaran un `INSERT` bajo una transacci√≥n marcada como de `solo lectura`.

### ‚öôÔ∏è Entorno de Pruebas

- Spring Boot: 3.5.3
- Spring Data R2DBC: versi√≥n incluida en Spring Boot 3.5.3
- Driver R2DBC: io.r2dbc:r2dbc-postgresql
- Base de datos: PostgreSQL
- Configuraci√≥n adicional:
    - √önicamente propiedades de conexi√≥n en application.yml
    - Logs SQL habilitados
    - Sin TransactionManager personalizado

### üß™ Pruebas realizadas

A continuaci√≥n se detallan las pruebas y resultados.

### üü¢ 1Ô∏è‚É£ Prueba `SIN` `@Transactional(readOnly = true)`

üìå C√≥digo del m√©todo

````java

@Override
public Flux<AuthorResponse> findAllAuthors() {
    return this.authorRepository.save(Author.builder()
                    .firstName("Ale")
                    .lastName("Flo")
                    .birthdate(LocalDate.parse("2025-05-06"))
                    .build())
            .doOnNext(author -> log.info("{}", author))
            .flatMapMany(author -> this.authorRepository.findAll())
            .map(this.authorMapper::toAuthorResponse);
}
````

üìã Resultado observado

- Se ejecut√≥ correctamente un INSERT.
- Luego se ejecut√≥ el SELECT de todos los autores.
- La respuesta incluy√≥ el autor reci√©n insertado.
- No se produjo ning√∫n error.

Log en el Ide

````bash
io.r2dbc.postgresql.PARAM                : Bind parameter [0] to: Ale
io.r2dbc.postgresql.PARAM                : Bind parameter [1] to: Flo
io.r2dbc.postgresql.PARAM                : Bind parameter [2] to: 2025-05-06
io.r2dbc.postgresql.QUERY                : Executing query: INSERT INTO authors (first_name, last_name, birthdate) VALUES ($1, $2, $3) RETURNING id
d.m.r.a.service.impl.AuthorServiceImpl   : Author(id=5, firstName=Ale, lastName=Flo, birthdate=2025-05-06)
io.r2dbc.postgresql.QUERY                : Executing query: SELECT authors.* FROM authors
````

Log en el cliente

````bash
$ curl -v http://localhost:8080/api/v1/authors/stream
>
< HTTP/1.1 200 OK
< transfer-encoding: chunked
< Content-Type: text/event-stream;charset=UTF-8
<
data:{"id":1,"firstName":"Milagros","lastName":"D√≠az","birthdate":"2006-06-15"}

data:{"id":2,"firstName":"Lesly","lastName":"√Åguila","birthdate":"1995-06-09"}

data:{"id":3,"firstName":"Kiara","lastName":"Lozano","birthdate":"2001-10-03"}

data:{"id":4,"firstName":"Briela","lastName":"Cirilo","birthdate":"1997-09-25"}

data:{"id":5,"firstName":"Ale","lastName":"Flo","birthdate":"2025-05-06"}
````

‚úÖ Conclusi√≥n:
> El m√©todo `sin readOnly` permite insertar datos normalmente.

### üîµ 2Ô∏è‚É£ Prueba `CON` `@Transactional(readOnly = true)`

üìå C√≥digo del m√©todo

````java

@Override
@Transactional(readOnly = true)
public Flux<AuthorResponse> findAllAuthors() {
    return this.authorRepository.save(Author.builder()
                    .firstName("Ale")
                    .lastName("Flo")
                    .birthdate(LocalDate.parse("2025-05-06"))
                    .build())
            .doOnNext(author -> log.info("{}", author))
            .flatMapMany(author -> this.authorRepository.findAll())
            .map(this.authorMapper::toAuthorResponse);
}
````

üìã Resultado observado

- La transacci√≥n inici√≥ en modo `READ ONLY`.
- PostgreSQL `bloque√≥ la escritura`.
- Se produjo un `rollback` autom√°tico.
- La llamada devolvi√≥ `error HTTP 500`.

Log en el ide

````bash
DEBUG 17820 --- io.r2dbc.postgresql.QUERY                : Executing query: BEGIN READ ONLY
DEBUG 17820 --- io.r2dbc.postgresql.PARAM                : Bind parameter [0] to: Ale
DEBUG 17820 --- io.r2dbc.postgresql.PARAM                : Bind parameter [1] to: Flo
DEBUG 17820 --- io.r2dbc.postgresql.PARAM                : Bind parameter [2] to: 2025-05-06
DEBUG 17820 --- io.r2dbc.postgresql.QUERY                : Executing query: INSERT INTO authors (first_name, last_name, birthdate) VALUES ($1, $2, $3) RETURNING id
DEBUG 17820 --- io.r2dbc.postgresql.QUERY                : Executing query: ROLLBACK
ERROR 17820 --- a.w.r.e.AbstractErrorWebExceptionHandler : [35475d0d-4]  500 Server Error for HTTP GET "/api/v1/authors/stream"

org.springframework.dao.DataAccessResourceFailureException: executeMany; SQL [INSERT INTO authors (first_name, last_name, birthdate) VALUES ($1, $2, $3)]; no se puede ejecutar INSERT en una transacci√≥n de s√≥lo lectura
````

Log en el cliente

````bash
$ curl -v http://localhost:8080/api/v1/authors/stream | jq
>
< HTTP/1.1 500 Internal Server Error
< Content-Type: application/json
< Content-Length: 319
<
{
  "timestamp": "2025-06-29T00:35:33.603+00:00",
  "path": "/api/v1/authors/stream",
  "status": 500,
  "error": "Internal Server Error",
  "requestId": "35475d0d-4",
  "message": "executeMany; SQL [INSERT INTO authors (first_name, last_name, birthdate) VALUES ($1, $2, $3)]; no se puede ejecutar INSERT en una transacci√≥n de s√≥lo lectura"
}
````

‚úÖ Conclusi√≥n:
> `@Transactional(readOnly = true)` se traduce correctamente en `BEGIN READ ONLY` y
> `PostgreSQL bloquea cualquier INSERT`.

### ‚ú® Conclusiones finales

1. La anotaci√≥n `@Transactional` en aplicaciones reactivas con `Spring WebFlux` y `R2DBC` funciona correctamente para
   delimitar transacciones que modifican la base de datos (`INSERT`, `UPDATE`, `DELETE`).
2. La anotaci√≥n `@Transactional(readOnly = true)` tambi√©n funciona correctamente, al iniciar la transacci√≥n en modo
   `READ ONLY` en `PostgreSQL`.
3. Al intentar insertar datos dentro de una transacci√≥n de solo lectura, `PostgreSQL` bloquea la operaci√≥n con un error
   claro, y `Spring` realiza un `rollback autom√°tico`.
4. Estas pruebas demuestran de forma emp√≠rica que el soporte de `readOnly=true` est√° operativo en `Spring Boot 3.5.x`
   sin necesidad de configuraciones adicionales.

üîî Importante

- `R2DBC + PostgreSQL`: la restricci√≥n de solo lectura la hace la base de datos.
- `JPA + Hibernate`: la restricci√≥n de solo lectura la hace Hibernate en la sesi√≥n.
- No todas las bases de datos se comportan igual.
- Siempre es buena pr√°ctica declarar la intenci√≥n con `readOnly = true`.

---

# Pruebas de Integraci√≥n

---

## Prueba de Integraci√≥n a repositorio

Vamos a crear dos archivos en nuestro classpath de `/test` que contendr√°n las instrucciones sql que ejecutaremos en cada
m√©todo de test de nuestro repositorio.

`src/test/resources/sql/data.sql`

````sql
INSERT INTO authors(first_name, last_name, birthdate)
VALUES
('Bel√©n', 'Velez', '2006-06-15'),
('Marco', 'Salvador', '1995-06-09'),
('Greys', 'Briones', '2001-10-03'),
('Luis', 'S√°nchez', '1997-09-25');

INSERT INTO books(title, publication_date, online_availability)
VALUES
('Los r√≠os profundos', '1999-01-15', true),
('La ciudad y los perros', '1985-03-18', true),
('El zorro de arriba y el zorro de abajo', '2002-05-06', false),
('Redoble por Rancas', '1988-07-15', true);

-- Book 1: tiene como author a Bel√©n y Marco
INSERT INTO book_authors(book_id, author_id)
VALUES
(1, 1),
(1, 2);

-- Book 2: tiene como author a Greys
INSERT INTO book_authors(book_id, author_id)
VALUES
(2, 3);
````

`src/test/resources/sql/reset_test_data.sql`

````sql
TRUNCATE TABLE book_authors RESTART IDENTITY CASCADE;
TRUNCATE TABLE books RESTART IDENTITY CASCADE;
TRUNCATE TABLE authors RESTART IDENTITY CASCADE;
````

La siguiente clase abstracta sirve como `base de configuraci√≥n` com√∫n para los tests que usen `@DataR2dbcTest`,
facilitando:

- La carga y ejecuci√≥n de scripts SQL antes de cada test para tener una base de datos en estado limpio.
- La inyecci√≥n autom√°tica del DatabaseClient para ejecutar directamente sentencias SQL reactivas.
- Reutilizaci√≥n de l√≥gica para inicializar datos comunes de prueba (`data.sql` y `reset_test_data.sql`).

La anotaci√≥n `@DataR2dbcTest`:

- Habilita solo los componentes de persistencia reactiva necesarios para probar con `R2DBC`.
- Configura una base de datos embebida o el `datasource configurado`.
- Excluye componentes Web, Beans externos, Controllers, etc.

> ‚úÖ Ideal para pruebas de repositorios (`ReactiveCrudRepository`, `R2dbcEntityTemplate`, etc.) de forma r√°pida y
> aislada.

La inyecci√≥n de `DatabaseClient`:

- Permite ejecutar queries SQL de forma reactiva (no bloqueante).
- Es √∫til para cargar directamente scripts SQL que no est√°n acoplados a entidades.

````java

@DataR2dbcTest
public abstract class AbstractTest {

    @Autowired
    private DatabaseClient databaseClient;
    private static String dataSQL;
    private static String resetTestData;

    @BeforeAll
    static void beforeAll() throws IOException {
        resetTestData = getSQL(new ClassPathResource("sql/reset_test_data.sql"));
        dataSQL = getSQL(new ClassPathResource("sql/data.sql"));
    }

    @BeforeEach
    void setUp() {
        this.executeSQL(resetTestData);
        this.executeSQL(dataSQL);
    }

    private static String getSQL(ClassPathResource resource) throws IOException {
        try (InputStream inputStream = resource.getInputStream()) {
            return new String(inputStream.readAllBytes(), StandardCharsets.UTF_8);
        }
    }

    private void executeSQL(String sql) {
        this.databaseClient.sql(sql)
                .fetch()
                .rowsUpdated()
                .block(); //Aunque se usa block(), est√° justificado aqu√≠ porque est√°s en una clase de prueba que no necesita mantener la naturaleza reactiva pura.
    }
}
````

La clase `AbstractTest` sirve como clase base para los test de repositorios reactivos con R2DBC. Utiliza la anotaci√≥n
`@DataR2dbcTest` para limitar el contexto a los beans de persistencia necesarios, acelerando la ejecuci√≥n de pruebas.

Antes de todos los tests, se cargan los scripts `reset_test_data.sql` y `data.sql` desde el classpath.
Luego, antes de cada m√©todo de test, se ejecutan estos scripts para garantizar un entorno limpio y reproducible. Esto
asegura que los tests no dependan del orden de ejecuci√≥n o del estado de datos compartido.

Finalmente, creamos la clase principal de pruebas para nuestro repositorio `AuthorRepositoryTest`. Esta clase hereda
las configuraciones que definimos en la clase abstracta anterior. Entonces:

- Hereda la configuraci√≥n base de `AbstractTest`.
    - Carga y resetea datos antes de cada test.
    - Usa un contexto limitado de `@DataR2dbcTest`.

````java

@Slf4j
class AuthorRepositoryTest extends AbstractTest {

    @Autowired
    private AuthorRepository authorRepository;

    @Test
    void findAllAuthors() {
        this.authorRepository.findAllAuthorsByIdIn(List.of(2, 3))
                .doOnNext(author -> log.info("{}", author))
                .as(StepVerifier::create)
                .assertNext(author -> {
                    Assertions.assertEquals(2, author.getId());
                    Assertions.assertEquals("Marco", author.getFirstName());
                    Assertions.assertEquals("Salvador", author.getLastName());
                    Assertions.assertEquals(LocalDate.parse("1995-06-09"), author.getBirthdate());
                })
                .assertNext(author -> {
                    Assertions.assertEquals(3, author.getId());
                    Assertions.assertEquals("Greys", author.getFirstName());
                    Assertions.assertEquals("Briones", author.getLastName());
                    Assertions.assertEquals(LocalDate.parse("2001-10-03"), author.getBirthdate());
                })
                .verifyComplete();
    }

    @Test
    void findAuthorById() {
        this.authorRepository.findAuthorById(1)
                .doOnNext(authorProjection -> log.info("{}", authorProjection))
                .as(StepVerifier::create)
                .assertNext(authorProjection -> {
                    Assertions.assertEquals("Bel√©n", authorProjection.getFirstName());
                    Assertions.assertEquals("Velez", authorProjection.getLastName());
                    Assertions.assertEquals(LocalDate.parse("2006-06-15"), authorProjection.getBirthdate());

                    // Comprobamos que el m√©todo por defecto de la proyecci√≥n est√° funcionando
                    Assertions.assertEquals("Bel√©n Velez", authorProjection.getFullName());
                })
                .verifyComplete();
    }

    @Test
    void findCountByQuery() {
        this.authorRepository.findCountByQuery("e")
                .as(StepVerifier::create)
                .assertNext(count -> Assertions.assertEquals(3, count))
                .verifyComplete();
    }

    @Test
    void findByQueryAndPageable() {
        this.authorRepository.findByQuery("e", PageRequest.of(0, 2))
                .doOnNext(authorProjection -> log.info("{}", authorProjection))
                .as(StepVerifier::create)
                .assertNext(authorProjection -> {
                    Assertions.assertEquals("Bel√©n", authorProjection.getFirstName());
                    Assertions.assertEquals("Velez", authorProjection.getLastName());
                    Assertions.assertEquals(LocalDate.parse("2006-06-15"), authorProjection.getBirthdate());
                    //default method
                    Assertions.assertEquals("Bel√©n Velez", authorProjection.getFullName());
                })
                .assertNext(authorProjection -> {
                    Assertions.assertEquals("Greys", authorProjection.getFirstName());
                    Assertions.assertEquals("Briones", authorProjection.getLastName());
                    Assertions.assertEquals(LocalDate.parse("2001-10-03"), authorProjection.getBirthdate());
                    //default method
                    Assertions.assertEquals("Greys Briones", authorProjection.getFullName());
                })
                .verifyComplete();
    }

    @Test
    void saveAuthor() {
        Author author = Author.builder()
                .firstName("Susana")
                .lastName("Alvarado")
                .birthdate(LocalDate.parse("2000-11-07"))
                .build();
        this.authorRepository.saveAuthor(author)
                .doOnNext(affectedRows -> log.info("affectedRows: {}", affectedRows))
                .as(StepVerifier::create)
                .assertNext(affectedRows -> Assertions.assertEquals(1, affectedRows))
                .verifyComplete();

        this.authorRepository.count()
                .as(StepVerifier::create)
                .expectNext(5L)
                .verifyComplete();

        this.authorRepository.findById(5)
                .doOnNext(authorRetrieved -> log.info("{}", authorRetrieved))
                .as(StepVerifier::create)
                .assertNext(authorRetrieved -> {
                    Assertions.assertNotNull(authorRetrieved.getId());
                    Assertions.assertEquals("Susana", authorRetrieved.getFirstName());
                    Assertions.assertEquals("Alvarado", authorRetrieved.getLastName());
                    Assertions.assertEquals(LocalDate.parse("2000-11-07"), authorRetrieved.getBirthdate());
                })
                .verifyComplete();

        this.authorRepository.deleteById(5)
                .then(this.authorRepository.count())
                .as(StepVerifier::create)
                .expectNext(4L)
                .verifyComplete();
    }

    @Test
    void updateCustomer() {
        this.authorRepository.findById(1)
                .doOnNext(author -> log.info("{}", author))
                .doOnNext(author -> author.setFirstName("Belencita"))
                .flatMap(author -> this.authorRepository.updateAuthor(author))
                .as(StepVerifier::create)
                .assertNext(affectedRows -> Assertions.assertEquals(1, affectedRows))
                .verifyComplete();

        this.authorRepository.findById(1)
                .doOnNext(author -> log.info("{}", author))
                .as(StepVerifier::create)
                .assertNext(author -> Assertions.assertEquals("Belencita", author.getFirstName()))
                .verifyComplete();
    }
}
````

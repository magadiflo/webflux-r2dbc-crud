# [WebFlux R2DBC Crud using PostgreSQL](https://www.youtube.com/watch?v=s6qKE0FD3BU&t=2137s)

- Proyecto tomado del canal de youtube de `Joas Dev`.
- Este proyecto est√° actualizado (25/06/2025) con algunos detalles que se vieron en el curso de
  [java-reactive-programming](https://github.com/magadiflo/java-reactive-programming.git) y de
  [webFlux-masterclass-microservices](https://github.com/magadiflo/webFlux-masterclass-microservices.git).

---

## Dependencias

A continuaci√≥n se muestran las dependencias utilizadas en este proyecto, de las cuales, la √∫nica dependencia que
agregamos manualmente fue [MapStruct](https://mapstruct.org/), las dem√°s dependencias las agregamos desde
[Spring Initializr (ver dependencias)](https://start.spring.io/#!type=maven-project&language=java&platformVersion=3.5.3&packaging=jar&jvmVersion=21&groupId=dev.magadiflo&artifactId=webflux-r2dbc-crud&name=webflux-r2dbc-crud&description=Demo%20project%20for%20Spring%20WebFlux%20with%20r2dbc%20PostgreSQL&packageName=dev.magadiflo.r2dbc.app&dependencies=webflux,data-r2dbc,postgresql,lombok).

````xml
<!--Spring Boot 3.5.3-->
<!--Java 21-->
<!--org.mapstruct.version 1.6.3-->
<!--lombok-mapstruct-binding.version 0.2.0-->
<dependencies>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-data-r2dbc</artifactId>
    </dependency>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-webflux</artifactId>
    </dependency>

    <!--Agregado manualmente-->
    <dependency>
        <groupId>org.mapstruct</groupId>
        <artifactId>mapstruct</artifactId>
        <version>${org.mapstruct.version}</version>
    </dependency>
    <!--/Agregado manualmente-->
    <dependency>
        <groupId>org.postgresql</groupId>
        <artifactId>postgresql</artifactId>
        <scope>runtime</scope>
    </dependency>
    <dependency>
        <groupId>org.postgresql</groupId>
        <artifactId>r2dbc-postgresql</artifactId>
        <scope>runtime</scope>
    </dependency>
    <dependency>
        <groupId>org.projectlombok</groupId>
        <artifactId>lombok</artifactId>
        <optional>true</optional>
    </dependency>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-test</artifactId>
        <scope>test</scope>
    </dependency>
    <dependency>
        <groupId>io.projectreactor</groupId>
        <artifactId>reactor-test</artifactId>
        <scope>test</scope>
    </dependency>
</dependencies>
````

### Procesadores de anotaciones

- [Using MapStruct with Maven and Lombok.](https://bootify.io/spring-data/mapstruct-with-maven-and-lombok.html)
- [Using MapStruct With Lombok](https://www.baeldung.com/java-mapstruct-lombok)

Como vamos a trabajar con `MapStruct` necesitamos ampliar el `maven-compiler-plugin` para activar la generaci√≥n de
c√≥digo de `MapStruct`. Observar que nuestro primer procesador de anotaciones es `Lombok`, seguido directamente por
`MapStruct`. Se requiere otra referencia a `lombok-mapstruct-binding` para que estas dos bibliotecas funcionen juntas.
Sin `Lombok`, solo se necesitar√≠a el `mapstruct-processor` en este momento.

````xml

<plugins>
    <!--MapStruct-->
    <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-compiler-plugin</artifactId>
        <version>${maven-compiler-plugin.version}</version>
        <configuration>
            <source>${java.version}</source>
            <target>${java.version}</target>
            <annotationProcessorPaths>
                <path>
                    <groupId>org.projectlombok</groupId>
                    <artifactId>lombok</artifactId>
                    <version>${lombok.version}</version>
                </path>
                <path>
                    <groupId>org.mapstruct</groupId>
                    <artifactId>mapstruct-processor</artifactId>
                    <version>${org.mapstruct.version}</version>
                </path>
                <path>
                    <groupId>org.projectlombok</groupId>
                    <artifactId>lombok-mapstruct-binding</artifactId>
                    <version>${lombok-mapstruct-binding.version}</version>
                </path>
            </annotationProcessorPaths>
        </configuration>
    </plugin>
    <!--/MapStruct-->
</plugins>
````

Es f√°cil cometer errores aqu√≠, ya que los procesadores de anotaciones son una funci√≥n avanzada. El principal error es
olvidar que nuestro entorno de ejecuci√≥n buscar√° procesadores de anotaciones en el `path` o en el `classpath`, pero no
en ambas.

Debemos tener en cuenta que, a partir de la versi√≥n `1.18.16` de `Lombok` hacia arriba, necesitamos agregar tanto la
dependencia `lombok-mapstruct-binding` de `Lombok` como la dependencia `mapstruct-processor` en el elemento
`annotationProcessorPaths`. Si no lo hacemos, podr√≠amos obtener un error de compilaci√≥n:
`‚ÄúPropiedad desconocida en el tipo de resultado‚Ä¶‚Äù`.

Necesitamos la dependencia `lombok-mapstruct-binding` para que `Lombok` y `MapStruct` funcionen juntos. En esencia,
le indica a `MapStruct` que espere hasta que `Lombok` haya completado todo el procesamiento de anotaciones antes
de generar clases de mapeador para los beans mejorados con `Lombok`.

## MapStruct vs ModelMapper

> En el proyecto original del tutorial de `Joas Dev` se usa como mapeador a `ModelMapper`, pero en mi caso, opt√© por
> usar `MapStruct` debido a las siguientes razones.

### ‚öôÔ∏è MapStruct

- `Rendimiento superior`: Genera c√≥digo de mapeo en tiempo de compilaci√≥n, `evitando el uso de reflexi√≥n` y mejorando
  la velocidad.
- `Detecci√≥n temprana de errores`: Al generar c√≥digo en compilaci√≥n, los errores de mapeo se detectan antes de ejecutar
  la aplicaci√≥n.
- `Menor uso de memoria`: No usa reflection, lo que reduce la presi√≥n sobre el garbage collector.
- `Ideal para proyectos grandes o cr√≠ticos en rendimiento`, como los que suelen construirse con `WebFlux`.
- `M√°s configuraci√≥n inicial`, ya que debes definir interfaces y m√©todos de mapeo expl√≠citamente.

### üîÑ ModelMapper

- `M√°s f√°cil de usar al principio`: Mapea autom√°ticamente campos con nombres similares `usando reflexi√≥n`.
- Puede tener problemas con tipos gen√©ricos complejos de los streams reactivos
- `Menor esfuerzo inicial`, √∫til para prototipos o proyectos peque√±os.
- `Menor rendimiento`: La `reflexi√≥n` introduce una sobrecarga que puede ser significativa en aplicaciones reactivas.
- `Menos control` sobre el proceso de mapeo, lo que puede dificultar el mantenimiento a largo plazo.
- Los errores de configuraci√≥n solo se detectan en runtime.
- Mayor consumo de memoria y CPU.

### ‚úÖ Conclusi√≥n

Dado el enfoque en rendimiento, claridad y mantenimiento en sistemas reactivamente transaccionales, `MapStruct` encaja
mejor con el estilo y los objetivos. Adem√°s, al generar c√≥digo expl√≠cito, facilita la documentaci√≥n y el control del
flujo de datos.

## Creando base de datos

Creamos la base de datos `db_webflux_r2dbc` cuyo usuario es `postgres` y la contrase√±a es `magadiflo`.

![01.png](assets/01.png)

## Configurando base de datos

En el `application.yml` agregamos las siguientes configuraciones.

````yml
server:
  port: 8080
  error:
    include-message: always

spring:
  application:
    name: webflux-r2dbc-crud
  r2dbc:
    url: r2dbc:postgresql://localhost:5432/db_webflux_r2dbc
    username: postgres
    password: magadiflo
````

La URL de conexi√≥n est√° configurada en `r2dbc:postgresql://localhost:5432/db_webflux_r2dbc`, donde:

- `r2dbc`, indicamos que usaremos `r2dbc` para conectarnos a la base de datos. Recordar que cuando usamos una
  aplicaci√≥n tradicional como `Spring Boot MVC` (no reactiva) usamos `jdbc`.
- `db_webflux_r2dbc`, es el nombre de la base de datos.
- `5432`, es el puerto por defecto `PostgreSQL`.

Las propiedades `spring.r2dbc.username` y `spring.r2dbc.password` proporcionan las credenciales para conectarse a
la base de datos.

## Habilitando logging para ver los queries y parameters en las consultas a PostgreSQL

En el `application.yml` agregamos las siguientes configuraciones para poder observar qu√© instrucciones sql se est√°n
ejecutando y qu√© par√°metros se est√°n enviando.

````yml
logging:
  level:
    io.r2dbc.postgresql.QUERY: DEBUG
    io.r2dbc.postgresql.PARAM: DEBUG
````

## Creando el esquema y datos de nuestra base de datos

Creamos el siguiente directorio `src/main/resources/sql` en nuestro proyecto. Aqu√≠ creamos el archivo `scheme.sql`
donde definimos las tablas `authors`, `books` y su relaci√≥n de muchos a muchos `book_authors`.

Definimos las instrucciones `DROP TABLE...` al inicio de este script para que la aplicaci√≥n inicie limpia y siempre
con los datos iniciales del archivo `data.sql` que crearemos m√°s adelante.

````sql
DROP TABLE IF EXISTS book_authors;
DROP TABLE IF EXISTS authors;
DROP TABLE IF EXISTS books;

CREATE TABLE authors(
    id SERIAL,
    first_name VARCHAR(45) NOT NULL,
    last_name VARCHAR(45) NOT NULL,
    birthdate DATE NOT NULL,
    CONSTRAINT pk_authors PRIMARY KEY(id)
);

CREATE TABLE books(
    id SERIAL,
    title VARCHAR(255) NOT NULL,
    publication_date DATE NOT NULL,
    online_availability BOOLEAN DEFAULT FALSE,
    CONSTRAINT pk_books PRIMARY KEY(id)
);

CREATE TABLE book_authors(
    book_id INTEGER NOT NULL,
    author_id INTEGER NOT NULL,
    CONSTRAINT fk_books_book_authors FOREIGN KEY(book_id) REFERENCES books(id),
    CONSTRAINT fk_authors_book_authors FOREIGN KEY(author_id) REFERENCES authors(id),
    CONSTRAINT pk_book_authors PRIMARY KEY(book_id, author_id)
);
````

Creamos el script `data.sql` en el mismo directorio que el `scheme.sql`. En este script `data.sql` vamos a definir
los valores iniciales con las que iniciar√° nuestra aplicaci√≥n cada vez que sea ejecutada.

````sql
INSERT INTO authors(first_name, last_name, birthdate)
VALUES
('Milagros', 'D√≠az', '2006-06-15'),
('Lesly', '√Åguila', '1995-06-09'),
('Kiara', 'Lozano', '2001-10-03'),
('Briela', 'Cirilo', '1997-09-25');

INSERT INTO books(title, publication_date, online_availability)
VALUES
('Cien a√±os de soledad', '1999-01-15', true),
('Paco Yunque', '1985-03-18', true),
('Los perros hambrientos', '2002-05-06', false),
('Edipo Rey', '1988-07-15', true);

-- Book 1: tiene como author a Milagros y Lesly
INSERT INTO book_authors(book_id, author_id)
VALUES
(1, 1),
(1, 2);

-- Book 2: tiene como author a Kiara
INSERT INTO book_authors(book_id, author_id)
VALUES
(2, 3);
````

## Inicializando Scheme y Data

Crearemos una clase de configuraci√≥n en `dev/magadiflo/r2dbc/app/config/DatabaseInitConfig.java` donde definiremos un
`@Bean` que nos retornar√° un objeto del tipo `ConnectionFactoryInitializer`.

`Spring Data R2DBC` `ConnectionFactoryInitializer` proporciona una manera conveniente de configurar e inicializar una
f√°brica de conexiones para una conexi√≥n de base de datos reactiva en una aplicaci√≥n `Spring`. Escanear√° el
`scheme.sql` y el `data.sql` en el `classpath` y ejecutar√° los `scripts SQL` para inicializar la base de datos cuando
la base de datos est√© conectada.

````java

@Configuration
public class DatabaseInitConfig {
    @Bean
    public ConnectionFactoryInitializer initializer(ConnectionFactory connectionFactory) {
        ClassPathResource schema = new ClassPathResource("sql/scheme.sql");
        ClassPathResource data = new ClassPathResource("sql/data.sql");
        ResourceDatabasePopulator resourceDatabasePopulator = new ResourceDatabasePopulator(schema, data);

        ConnectionFactoryInitializer initializer = new ConnectionFactoryInitializer();
        initializer.setConnectionFactory(connectionFactory);
        initializer.setDatabasePopulator(resourceDatabasePopulator);
        return initializer;
    }
}
````

Si hasta este punto ejecutamos la aplicaci√≥n, veremos que el log nos muestra una ejecuci√≥n exitosa.

````bash
  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/

 :: Spring Boot ::                (v3.5.3)

2025-06-26T12:00:08.010-05:00  INFO 3520 --- [webflux-r2dbc-crud] [           main] d.m.r.app.WebfluxR2dbcCrudApplication    : Starting WebfluxR2dbcCrudApplication using Java 21.0.6 with PID 3520 (D:\programming\spring\02.youtube\10.joas_dev\webflux-r2dbc-crud\target\classes started by magadiflo in D:\programming\spring\02.youtube\10.joas_dev\webflux-r2dbc-crud)
2025-06-26T12:00:08.016-05:00  INFO 3520 --- [webflux-r2dbc-crud] [           main] d.m.r.app.WebfluxR2dbcCrudApplication    : No active profile set, falling back to 1 default profile: "default"
2025-06-26T12:00:09.038-05:00  INFO 3520 --- [webflux-r2dbc-crud] [           main] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data R2DBC repositories in DEFAULT mode.
2025-06-26T12:00:09.074-05:00  INFO 3520 --- [webflux-r2dbc-crud] [           main] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 18 ms. Found 0 R2DBC repository interfaces.
2025-06-26T12:00:10.703-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-1] io.r2dbc.postgresql.QUERY                : Executing query: SHOW TRANSACTION ISOLATION LEVEL
2025-06-26T12:00:10.714-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-1] io.r2dbc.postgresql.QUERY                : Executing query: SELECT oid, * FROM pg_catalog.pg_type WHERE typname IN ('hstore','geometry','vector')
2025-06-26T12:00:10.787-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [      Thread-16] io.r2dbc.postgresql.QUERY                : Executing query: DROP TABLE IF EXISTS book_authors
2025-06-26T12:00:10.796-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-2] io.r2dbc.postgresql.QUERY                : Executing query: SHOW TRANSACTION ISOLATION LEVEL
2025-06-26T12:00:10.798-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-2] io.r2dbc.postgresql.QUERY                : Executing query: SELECT oid, * FROM pg_catalog.pg_type WHERE typname IN ('hstore','geometry','vector')
2025-06-26T12:00:10.818-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-1] io.r2dbc.postgresql.QUERY                : Executing query: DROP TABLE IF EXISTS authors
2025-06-26T12:00:10.829-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-1] io.r2dbc.postgresql.QUERY                : Executing query: DROP TABLE IF EXISTS books
2025-06-26T12:00:10.836-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-1] io.r2dbc.postgresql.QUERY                : Executing query: CREATE TABLE authors( id SERIAL, first_name VARCHAR(45) NOT NULL, last_name VARCHAR(45) NOT NULL, birthdate DATE NOT NULL, CONSTRAINT pk_authors PRIMARY KEY(id) )
2025-06-26T12:00:10.860-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-3] io.r2dbc.postgresql.QUERY                : Executing query: SHOW TRANSACTION ISOLATION LEVEL
2025-06-26T12:00:10.861-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-3] io.r2dbc.postgresql.QUERY                : Executing query: SELECT oid, * FROM pg_catalog.pg_type WHERE typname IN ('hstore','geometry','vector')
2025-06-26T12:00:10.875-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-1] io.r2dbc.postgresql.QUERY                : Executing query: CREATE TABLE books( id SERIAL, title VARCHAR(255) NOT NULL, publication_date DATE NOT NULL, online_availability BOOLEAN DEFAULT FALSE, CONSTRAINT pk_books PRIMARY KEY(id) )
2025-06-26T12:00:10.882-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-1] io.r2dbc.postgresql.QUERY                : Executing query: CREATE TABLE book_authors( book_id INTEGER NOT NULL, author_id INTEGER NOT NULL, CONSTRAINT fk_books_book_authors FOREIGN KEY(book_id) REFERENCES books(id), CONSTRAINT fk_authors_book_authors FOREIGN KEY(author_id) REFERENCES authors(id), CONSTRAINT pk_book_authors PRIMARY KEY(book_id, author_id) )
2025-06-26T12:00:10.894-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [      Thread-17] io.r2dbc.postgresql.QUERY                : Executing query: INSERT INTO authors(first_name, last_name, birthdate) VALUES ('Milagros', 'D√≠az', '2006-06-15'), ('Lesly', '√Åguila', '1995-06-09'), ('Kiara', 'Lozano', '2001-10-03'), ('Briela', 'Cirilo', '1997-09-25')
2025-06-26T12:00:10.897-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-1] io.r2dbc.postgresql.QUERY                : Executing query: INSERT INTO books(title, publication_date, online_availability) VALUES ('Cien a√±os de soledad', '1999-01-15', true), ('Paco Yunque', '1985-03-18', true), ('Los perros hambrientos', '2002-05-06', false), ('Edipo Rey', '1988-07-15', true)
2025-06-26T12:00:10.900-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-1] io.r2dbc.postgresql.QUERY                : Executing query: INSERT INTO book_authors(book_id, author_id) VALUES (1, 1), (1, 2)
2025-06-26T12:00:10.903-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-1] io.r2dbc.postgresql.QUERY                : Executing query: INSERT INTO book_authors(book_id, author_id) VALUES (2, 3)
2025-06-26T12:00:10.948-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-4] io.r2dbc.postgresql.QUERY                : Executing query: SHOW TRANSACTION ISOLATION LEVEL
2025-06-26T12:00:10.950-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-4] io.r2dbc.postgresql.QUERY                : Executing query: SELECT oid, * FROM pg_catalog.pg_type WHERE typname IN ('hstore','geometry','vector')
2025-06-26T12:00:11.055-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-5] io.r2dbc.postgresql.QUERY                : Executing query: SHOW TRANSACTION ISOLATION LEVEL
2025-06-26T12:00:11.057-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-5] io.r2dbc.postgresql.QUERY                : Executing query: SELECT oid, * FROM pg_catalog.pg_type WHERE typname IN ('hstore','geometry','vector')
2025-06-26T12:00:11.128-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-6] io.r2dbc.postgresql.QUERY                : Executing query: SHOW TRANSACTION ISOLATION LEVEL
2025-06-26T12:00:11.130-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-6] io.r2dbc.postgresql.QUERY                : Executing query: SELECT oid, * FROM pg_catalog.pg_type WHERE typname IN ('hstore','geometry','vector')
2025-06-26T12:00:11.237-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-7] io.r2dbc.postgresql.QUERY                : Executing query: SHOW TRANSACTION ISOLATION LEVEL
2025-06-26T12:00:11.239-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-7] io.r2dbc.postgresql.QUERY                : Executing query: SELECT oid, * FROM pg_catalog.pg_type WHERE typname IN ('hstore','geometry','vector')
2025-06-26T12:00:11.343-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-8] io.r2dbc.postgresql.QUERY                : Executing query: SHOW TRANSACTION ISOLATION LEVEL
2025-06-26T12:00:11.344-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-8] io.r2dbc.postgresql.QUERY                : Executing query: SELECT oid, * FROM pg_catalog.pg_type WHERE typname IN ('hstore','geometry','vector')
2025-06-26T12:00:11.432-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-1] io.r2dbc.postgresql.QUERY                : Executing query: SHOW TRANSACTION ISOLATION LEVEL
2025-06-26T12:00:11.435-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-1] io.r2dbc.postgresql.QUERY                : Executing query: SELECT oid, * FROM pg_catalog.pg_type WHERE typname IN ('hstore','geometry','vector')
2025-06-26T12:00:11.546-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-2] io.r2dbc.postgresql.QUERY                : Executing query: SHOW TRANSACTION ISOLATION LEVEL
2025-06-26T12:00:11.548-05:00 DEBUG 3520 --- [webflux-r2dbc-crud] [actor-tcp-nio-2] io.r2dbc.postgresql.QUERY                : Executing query: SELECT oid, * FROM pg_catalog.pg_type WHERE typname IN ('hstore','geometry','vector')
2025-06-26T12:00:11.893-05:00  INFO 3520 --- [webflux-r2dbc-crud] [           main] o.s.b.web.embedded.netty.NettyWebServer  : Netty started on port 8080 (http)
2025-06-26T12:00:11.919-05:00  INFO 3520 --- [webflux-r2dbc-crud] [           main] d.m.r.app.WebfluxR2dbcCrudApplication    : Started WebfluxR2dbcCrudApplication in 4.713 seconds (process running for 6.126)
````

Y si ahora revisamos la base de datos `db_webflux_r2dbc` en postgres, veremos que las tablas se han creado
y poblado correctamente.

![02.png](assets/02.png)

## Creando entidades: Author, Book y BookAuthor

Creamos las entidades correspondientes a las tablas que definimos en el `scheme.sql`. Es importante recordar que aqu√≠
estamos trabajando con `R2DBC`, por lo tanto hay que tener algunas consideraciones:

- No tenemos la anotaci√≥n `@Entity` en `R2DBC`.
- Las anotaciones `@Table` o `@Column` realmente no son necesarios, pero si necesitamos agregar alguna personalizaci√≥n
  podemos usarlos. Por ejemplo, la anotaci√≥n `@Table` nos permite redefinir el nombre de la tabla de la base de datos
  para cada entidad.
- La anotaci√≥n `@Id` es necesaria para identificar la clave primaria de la entidad.
- A diferencia de `JPA`, `R2DBC` no maneja autom√°ticamente las relaciones entre entidades (no hay `@OneToMany`,
  `@ManyToOne`, etc.).

````java

@ToString
@AllArgsConstructor
@NoArgsConstructor
@Builder
@Data
@Table(name = "authors")
public class Author {
    @Id
    private Integer id;
    private String firstName;
    private String lastName;
    private LocalDate birthdate;
}
````

````java

@ToString
@AllArgsConstructor
@NoArgsConstructor
@Builder
@Data
@Table(name = "books")
public class Book {
    @Id
    private Integer id;
    private String title;
    private LocalDate publicationDate;
    private Boolean onlineAvailability;
}
````

````java

@ToString
@AllArgsConstructor
@NoArgsConstructor
@Builder
@Data
@Table(name = "book_authors")
public class BookAuthor {
    private Integer bookId;
    private Integer authorId;
}
````

### ‚ö†Ô∏è Nota sobre Relaciones Muchos a Muchos en R2DBC

La tabla `book_authors` representa una **relaci√≥n muchos a muchos** entre `Author` y `Book`:

- Un autor puede escribir muchos libros
- Un libro puede ser escrito por muchos autores

### Dise√±o de Base de Datos

Para esta relaci√≥n `M:N`, hemos implementado una **clave primaria compuesta** que sigue el modelo relacional puro:

```sql
CREATE TABLE book_authors(
    book_id INTEGER NOT NULL,
    author_id INTEGER NOT NULL,
    CONSTRAINT fk_books_book_authors FOREIGN KEY(book_id) REFERENCES books(id),
    CONSTRAINT fk_authors_book_authors FOREIGN KEY(author_id) REFERENCES authors(id),
    CONSTRAINT pk_book_authors PRIMARY KEY(book_id, author_id) --Clave primaria compuesta
);
```

Este dise√±o:

- ‚úÖ `Garantiza unicidad`: No permite relaciones duplicadas.
- ‚úÖ `Es sem√°nticamente correcto`: La PK representa exactamente la relaci√≥n.
- ‚úÖ `Sigue el modelo relacional puro`: Sin campos artificiales innecesarios.

### Limitaci√≥n de R2DBC

`Spring Data R2DBC no soporta claves primarias compuestas` (a diferencia de JPA que tiene @IdClass y @EmbeddedId).
Por esta raz√≥n:

1. No podemos usar `ReactiveCrudRepository` para esta entidad.
2. Debemos usar `DatabaseClient` con `SQL nativo` para las operaciones `CRUD`.
3. Creamos una entidad `BookAuthor` `sin anotaci√≥n @Id` para representar la tabla.

Esta aproximaci√≥n nos permite mantener el dise√±o te√≥ricamente correcto en base de datos mientras trabajamos con las
limitaciones actuales de `R2DBC`.

### ‚ö†Ô∏è Importante

> Si quisi√©ramos utilizar `ReactiveCrudRepository` con la entidad `BookAuthor`, ser√≠a necesario agregar una columna
> adicional en la tabla `book_authors`, como un identificador √∫nico (por ejemplo, `id SERIAL PRIMARY KEY`).
> Esta nueva columna actuar√≠a como clave primaria y permitir√≠a anotar un atributo correspondiente en la entidad
> con `@Id`.
>
> Sin embargo, al introducir una clave artificial, la tabla dejar√≠a de representar un modelo relacional
> puramente compuesto, rompiendo en cierta forma con la normalizaci√≥n cl√°sica de una relaci√≥n muchos a muchos.
>
> Este caso lo podemos ver en el proyecto
> [webFlux-masterclass-microservices](https://github.com/magadiflo/webFlux-masterclass-microservices/tree/main/projects/webflux-playground/src/main/java/dev/magadiflo/app/sec03/entity)

## Creando repositorios

A nuestras entidades `Author` y `Book` les crearemos a cada uno su interfaz de repositorio. Estos repositorios nos
permitir√°n interactuar con las tablas de la base de datos `authors` y `books`. Con respecto a la entidad `BookAuthor`,
esta la manejaremos dentro de una clase `dao` haciendo uso del `DatabaseClient`.

La interfaz `ReactiveCrudRepository` nos permitir√° usar sus m√©todos ya definidos, tales como el `save()`, `findById()`,
`findAll()`, `count()`, `delete()`, `deleteById()`, `deleteAll()`, etc.`

A continuaci√≥n se muestra la creaci√≥n del repositorio `BookRepository` para la entidad `Book`.

````java
public interface BookRepository extends ReactiveCrudRepository<Book, Integer> {
}
````

Antes de crear el repositorio `AuthorRepository` vamos a crear una proyecci√≥n basada en interfaz que luego la usaremos
en algunos m√©todos del repositorio.

Crearemos la interfaz de proyecci√≥n `AuthorProjection` que ser√° utilizada en `Spring Data R2DBC` para exponer datos
de la entidad `Author` de manera optimizada en respuestas JSON, evitando la necesidad de crear un `DTO` separado.

````java

@JsonPropertyOrder(value = {"firstName", "lastName", "fullName", "birthdate"})
public interface AuthorProjection {
    String getFirstName();

    String getLastName();

    LocalDate getBirthdate();

    default String getFullName() {
        if (Objects.isNull(getFirstName()) || Objects.isNull(getLastName())) {
            return "";
        }
        return "%s %s".formatted(getFirstName(), getLastName());
    }
}
````

Esta interfaz utiliza la anotaci√≥n `@JsonPropertyOrder(value = {"firstName", "lastName", "fullName", "birthdate"})`
para ordenar expl√≠citamente los campos en la respuesta JSON en el orden indicado.

Sin esta anotaci√≥n:

- Jackson no garantiza el orden de los campos.
- El orden puede cambiar entre ejecuciones, versiones de Spring, o versiones de Jackson.
- Campos default como fullName suelen aparecer en posiciones impredecibles.

Con esta anotaci√≥n:

- La respuesta JSON mantiene siempre un orden estable.
- Se mejora la claridad para clientes y frontends.
- Se facilita la validaci√≥n en tests automatizados.

### ‚ú® Nota importante

> La anotaci√≥n `@JsonPropertyOrder` solo es necesaria en `interfaces` de proyecci√≥n, ya que en estas el orden de las
> propiedades no est√° garantizado por defecto y puede variar.
>
> Si en lugar de una interfaz usamos un `record` o una `clase` DTO convencional, no necesitamos esta anotaci√≥n, ya que
> en un `record`, el orden de los campos en el JSON coincide con el orden en que se definen los componentes del record.
> En una clase, Jackson respeta el orden en que declares los atributos.

> M√°s adelante explicamos en detalle qu√© es esto de las `proyecciones`.

Ahora, mostramos la creaci√≥n del repositorio `AuthorRepository` para la entidad `Author`. Algo que vamos a hacer en
este repositorio es que a pesar de que el `ReactiveCrudRepository` ya viene con m√©todos predefinidos como el
`save()`, `findBYId()`, etc. en nuestro caso vamos a definir nuestros propios m√©todos para practicar un poco.

````java
public interface AuthorRepository extends ReactiveCrudRepository<Author, Integer> {
    /**
     * @param author entity
     * @return affectedRows
     */
    @Modifying
    @Query(value = """
            INSERT INTO authors(first_name, last_name, birthdate)
            VALUES(:#{#author.firstName}, :#{#author.lastName}, :#{#author.birthdate})
            """)
    Mono<Integer> saveAuthor(@Param("author") Author author);

    /**
     * @param author entity
     * @return affectedRows
     */
    @Modifying
    @Query(value = """
            UPDATE authors
            SET first_name = :#{#author.firstName},
                last_name = :#{#author.lastName},
                birthdate = :#{#author.birthdate}
            WHERE id = :#{#author.id}
            """)
    Mono<Integer> updateAuthor(Author author);

    @Query("""
            SELECT a.id, a.first_name, a.last_name, a.birthdate
            FROM authors AS a
            WHERE a.id IN(:authorIds)
            """)
    Flux<Author> findAllAuthorsByIdIn(List<Integer> authorIds);

    @Query("""
            SELECT COUNT(a.id)
            FROM authors AS a
            WHERE a.first_name LIKE :#{'%' + #query + '%'}
                OR a.last_name LIKE :#{'%' + #query + '%'}
            """)
    Mono<Integer> findCountByQuery(String query);

    @Query("""
            SELECT a.id, a.first_name, a.last_name, a.birthdate
            FROM authors AS a
            WHERE a.id = :authorId
            """)
    Mono<AuthorProjection> findAuthorById(Integer authorId);

    @Query("""
            SELECT a.id, a.first_name, a.last_name, a.birthdate
            FROM authors AS a
            WHERE a.first_name LIKE :#{'%' + #query + '%'}
                OR a.last_name LIKE :#{'%' + #query + '%'}
            ORDER BY a.id ASC
            LIMIT :#{#pageable.getPageSize()}
            OFFSET :#{#pageable.getOffset()}
            """)
    Flux<AuthorProjection> findByQuery(String query, Pageable pageable);

    @Modifying
    @Query("DELETE FROM authors AS a WHERE a.id = :authorId")
    Mono<Boolean> deleteAuthorById(Integer authorId);
}
````

En el repositorio `AuthorRepository` hemos definido m√©todos personalizados, donde:

- Usamos la anotaci√≥n `@Query()` para definir nuestra consulta.


- La consulta usada en la anotaci√≥n `@Query()` es `SQL nativo`, ya que estamos trabajando con `Spring Data R2DBC`
  y no con `Spring Data JPA`. Aunque dicho sea de paso, con `Spring Data JPA` tambi√©n se puede trabajar con `SQL nativo`
  solo que en ese caso es necesario agregar el atributo `nativeQuery` en la anotaci√≥n de la siguiente manera
  `@Query(value = "TU_CONSULTA_SQL_CON_JPA", nativeQuery = true)`, mientras que con `Spring Data R2DBC` usamos
  directamente `SQL nativo` en la anotaci√≥n `@Query`.


- La anotaci√≥n `@Modifying` indica que un m√©todo de consulta debe considerarse una consulta de modificaci√≥n que puede
  devolver:
    - `Void` para descartar el recuento de actualizaciones y esperar a que se complete.
    - `Integer` u otro tipo num√©rico que emite el recuento de filas afectadas.
    - `Boolean` para indicar si se actualiz√≥ al menos una fila.

  Los m√©todos de consulta anotados con `@Modifying` suelen ser instrucciones `INSERT, UPDATE, DELETE y DDL` que no
  devuelven resultados tabulares.


- Normalmente, cuando definimos par√°metros a nuestros m√©todos de repositorio, si son pocos par√°metros podemos definirlos
  uno a uno, pero si son muchos par√°metros, podemos pasarle directamente un objeto que tendr√° las propiedades que
  usaremos en la consulta. En nuestro caso, observemos la firma de nuestro m√©todo `saveAuthor()`
  `Mono<Integer> saveAuthor(@Param("author") Author author)`, le estamos pasando la clase `Author`.


- Para usar las propiedades del objeto pasado por par√°metro dentro de la consulta SQL usamos `SpEL`, por ejemplo:
  `:#{#author.firstName}`, donde `author` es el par√°metro definido en el m√©todo y `firstName` es la propiedad del
  objeto. Esta sintaxis se utiliza para acceder a expresiones `SpEL (Spring Expression Language)`. Permite referenciar
  propiedades y m√©todos de objetos directamente en la consulta.


- El prefijo `#{}` indica que se est√° utilizando `SpEL`, y el s√≠mbolo `#` se utiliza para acceder a los par√°metros del
  m√©todo o a las propiedades del objeto. Por ejemplo. `:#{#pageable.getPageSize()}` accede al m√©todo `getPageSize()` del
  objeto `Pageable` pasado como par√°metro.

Veamos un poco m√°s a detalle la siguiente consulta que hemos creado anteriormente.

````java

@Query("""
        SELECT a.first_name, a.last_name, a.birthdate
        FROM authors AS a
        WHERE a.first_name LIKE :#{'%' + #query + '%'}
            OR a.last_name LIKE :#{'%' + #query + '%'}
        ORDER BY a.id ASC
        LIMIT :#{#pageable.getPageSize()}
        OFFSET :#{#pageable.getOffset()}
        """)
Flux<AuthorProjection> findByQuery(String query, Pageable pageable);
````

Estamos pasando por par√°metro un `String query` y un `Pageable pageable`. Centr√©monos en el objeto `pageable`. Estamos
agregando este objeto `pageable` por par√°metro con la √∫nica finalidad de poder usar los valores internos que nos
proporcione su implementaci√≥n. En otras palabras, lo que pasamos por par√°metro al m√©todo `findByQuery(...)` es la
variable `query` y la implementaci√≥n de la interfaz `Pageable`. Esta implementaci√≥n la podemos obtener de un
`PageRequest.of(pageNumber, pageSize)`. Internamente, la implementaci√≥n hace ciertas operaciones, las mismas que
podemos obtenerlas, por ejemplo con el `getOffset()` que es la multiplicaci√≥n del `pageNumber * pageSize`.

Por otro lado, algo importante que se debe resaltar en las consultas personalizadas del repositorio anterior es que en
los m√©todos `findAuthorById` y `findByQuery` estamos usando el concepto de `Projections` (a modo de ejemplo), con
`projections` podemos recuperar del total de columnas que tenga una tabla, solo las columnas que queramos. Por ejemplo,
si nuestra tabla tuviera 50 columnas, con projections podemos recuperar solo 5 columnas, no todas, sino las que son
realmente necesarias.

A continuaci√≥n, veamos el tema m√°s detalladamente:

### [Projections](https://docs.spring.io/spring-data/jpa/reference/repositories/projections.html)

Los m√©todos de consulta de Spring Data generalmente devuelven una o varias instancias de la ra√≠z agregada administrada
por el repositorio. Sin embargo, a veces puede resultar conveniente crear proyecciones basadas en ciertos atributos de
esos tipos. `Spring Data` permite modelar tipos de retorno dedicados para recuperar de manera m√°s selectiva vistas
parciales de los agregados administrados.

> Supongamos que tenemos una entidad con m√∫chos atributos, unas 100 por ejemplo (por exagerar). Ahora, imagine que
> queremos recuperar √∫nicamente 3 atributos ¬øc√≥mo lo har√≠amos?

### Proyecciones basadas en interfaz

La forma m√°s sencilla de limitar el resultado de las consultas solo a los atributos seleccionados es declarando una
interfaz que exponga los m√©todos de acceso para que se lean las propiedades, como se muestra en el siguiente ejemplo:

Supongamos que tenemos el siguiente repositorio y su aggregate root:

````java
class Person {
    @Id
    UUID id;
    String firstname;
    String lastname;
    Address address;

    static class Address {
        String zipCode, city, street;
    }
}

interface PersonRepository extends Repository<Person, UUID> {
    Collection<Person> findByLastname(String lastname);
}
````

Ahora, usando **proyecciones basadas en interfaz** definimos √∫nicamente los atributos que queremos recuperar, por
ejemplo, recuperar √∫nicamente los atributos del nombre de la persona:

````java
interface NamesOnly {
    String getFirstname();

    String getLastname();
}
````

Lo importante aqu√≠ es que las propiedades definidas aqu√≠ coinciden exactamente con las propiedades del aggregate root.
Al hacerlo, se puede agregar un m√©todo de consulta de la siguiente manera:

````java
// Un repositorio que utiliza una proyecci√≥n basada en interfaz con un m√©todo de consulta
interface PersonRepository extends Repository<Person, UUID> {
    Collection<NamesOnly> findByLastname(String lastname);
}
````

#### Proyecciones cerradas

Una interfaz de proyecci√≥n cuyos m√©todos de acceso coinciden con las propiedades del agregado de destino se considera
una proyecci√≥n cerrada. El siguiente ejemplo (que tambi√©n utilizamos anteriormente en este cap√≠tulo) es una proyecci√≥n
cerrada.

````java
interface NamesOnly {
    String getFirstname();

    String getLastname();
}
````

#### Proyecciones abiertas

Los m√©todos de acceso en las interfaces de proyecci√≥n tambi√©n se pueden utilizar para calcular nuevos valores mediante
la anotaci√≥n @Value, como se muestra en el siguiente ejemplo:

````java
interface NamesOnly {
    @Value("#{target.firstname + ' ' + target.lastname}")
    String getFullName();
    /*...*/
}
````

La ra√≠z agregada que respalda la proyecci√≥n est√° disponible en la variable objetivo. Una interfaz de proyecci√≥n que
utiliza `@Value` es una `proyecci√≥n abierta`. Spring Data no puede aplicar optimizaciones de ejecuci√≥n de consultas en
este caso, porque la expresi√≥n `SpEL` podr√≠a usar cualquier atributo de la ra√≠z agregada.

Las expresiones utilizadas en `@Value` no deben ser demasiado complejas; debe evitar la programaci√≥n en variables de
cadena. Para expresiones muy simples, una opci√≥n podr√≠a ser recurrir a m√©todos predeterminados (introducidos en Java 8),
como se muestra en el siguiente ejemplo:

````java
// Una interfaz de proyecci√≥n que utiliza un m√©todo predeterminado para l√≥gica personalizada
interface NamesOnly {

    String getFirstname();

    String getLastname();

    default String getFullName() {
        return getFirstname().concat(" ").concat(getLastname());
    }
}
````

## Prueba de Integraci√≥n a repositorio

Vamos a crear dos archivos en nuestro classpath de `/test` que contendr√°n las instrucciones sql que ejecutaremos en cada
m√©todo de test de nuestro repositorio.

`src/test/resources/sql/data.sql`

````sql
INSERT INTO authors(first_name, last_name, birthdate)
VALUES
('Bel√©n', 'Velez', '2006-06-15'),
('Marco', 'Salvador', '1995-06-09'),
('Greys', 'Briones', '2001-10-03'),
('Luis', 'S√°nchez', '1997-09-25');

INSERT INTO books(title, publication_date, online_availability)
VALUES
('Los r√≠os profundos', '1999-01-15', true),
('La ciudad y los perros', '1985-03-18', true),
('El zorro de arriba y el zorro de abajo', '2002-05-06', false),
('Redoble por Rancas', '1988-07-15', true);

-- Book 1: tiene como author a Bel√©n y Marco
INSERT INTO book_authors(book_id, author_id)
VALUES
(1, 1),
(1, 2);

-- Book 2: tiene como author a Greys
INSERT INTO book_authors(book_id, author_id)
VALUES
(2, 3);
````

`src/test/resources/sql/reset_test_data.sql`

````sql
TRUNCATE TABLE book_authors RESTART IDENTITY CASCADE;
TRUNCATE TABLE books RESTART IDENTITY CASCADE;
TRUNCATE TABLE authors RESTART IDENTITY CASCADE;
````

La siguiente clase abstracta sirve como `base de configuraci√≥n` com√∫n para los tests que usen `@DataR2dbcTest`,
facilitando:

- La carga y ejecuci√≥n de scripts SQL antes de cada test para tener una base de datos en estado limpio.
- La inyecci√≥n autom√°tica del DatabaseClient para ejecutar directamente sentencias SQL reactivas.
- Reutilizaci√≥n de l√≥gica para inicializar datos comunes de prueba (`data.sql` y `reset_test_data.sql`).

La anotaci√≥n `@DataR2dbcTest`:

- Habilita solo los componentes de persistencia reactiva necesarios para probar con `R2DBC`.
- Configura una base de datos embebida o el `datasource configurado`.
- Excluye componentes Web, Beans externos, Controllers, etc.

> ‚úÖ Ideal para pruebas de repositorios (`ReactiveCrudRepository`, `R2dbcEntityTemplate`, etc.) de forma r√°pida y
> aislada.

La inyecci√≥n de `DatabaseClient`:

- Permite ejecutar queries SQL de forma reactiva (no bloqueante).
- Es √∫til para cargar directamente scripts SQL que no est√°n acoplados a entidades.

````java

@DataR2dbcTest
public abstract class AbstractTest {

    @Autowired
    private DatabaseClient databaseClient;
    private static String dataSQL;
    private static String resetTestData;

    @BeforeAll
    static void beforeAll() throws IOException {
        resetTestData = getSQL(new ClassPathResource("sql/reset_test_data.sql"));
        dataSQL = getSQL(new ClassPathResource("sql/data.sql"));
    }

    @BeforeEach
    void setUp() {
        this.executeSQL(resetTestData);
        this.executeSQL(dataSQL);
    }

    private static String getSQL(ClassPathResource resource) throws IOException {
        try (InputStream inputStream = resource.getInputStream()) {
            return new String(inputStream.readAllBytes(), StandardCharsets.UTF_8);
        }
    }

    private void executeSQL(String sql) {
        this.databaseClient.sql(sql)
                .fetch()
                .rowsUpdated()
                .block(); //Aunque se usa block(), est√° justificado aqu√≠ porque est√°s en una clase de prueba que no necesita mantener la naturaleza reactiva pura.
    }
}
````

La clase `AbstractTest` sirve como clase base para los test de repositorios reactivos con R2DBC. Utiliza la anotaci√≥n
`@DataR2dbcTest` para limitar el contexto a los beans de persistencia necesarios, acelerando la ejecuci√≥n de pruebas.

Antes de todos los tests, se cargan los scripts `reset_test_data.sql` y `data.sql` desde el classpath.
Luego, antes de cada m√©todo de test, se ejecutan estos scripts para garantizar un entorno limpio y reproducible. Esto
asegura que los tests no dependan del orden de ejecuci√≥n o del estado de datos compartido.

Finalmente, creamos la clase principal de pruebas para nuestro repositorio `AuthorRepositoryTest`. Esta clase hereda
las configuraciones que definimos en la clase abstracta anterior. Entonces:

- Hereda la configuraci√≥n base de `AbstractTest`.
    - Carga y resetea datos antes de cada test.
    - Usa un contexto limitado de `@DataR2dbcTest`.

````java

@Slf4j
class AuthorRepositoryTest extends AbstractTest {

    @Autowired
    private AuthorRepository authorRepository;

    @Test
    void findAllAuthors() {
        this.authorRepository.findAllAuthorsByIdIn(List.of(2, 3))
                .doOnNext(author -> log.info("{}", author))
                .as(StepVerifier::create)
                .assertNext(author -> {
                    Assertions.assertEquals(2, author.getId());
                    Assertions.assertEquals("Marco", author.getFirstName());
                    Assertions.assertEquals("Salvador", author.getLastName());
                    Assertions.assertEquals(LocalDate.parse("1995-06-09"), author.getBirthdate());
                })
                .assertNext(author -> {
                    Assertions.assertEquals(3, author.getId());
                    Assertions.assertEquals("Greys", author.getFirstName());
                    Assertions.assertEquals("Briones", author.getLastName());
                    Assertions.assertEquals(LocalDate.parse("2001-10-03"), author.getBirthdate());
                })
                .verifyComplete();
    }

    @Test
    void findAuthorById() {
        this.authorRepository.findAuthorById(1)
                .doOnNext(authorProjection -> log.info("{}", authorProjection))
                .as(StepVerifier::create)
                .assertNext(authorProjection -> {
                    Assertions.assertEquals("Bel√©n", authorProjection.getFirstName());
                    Assertions.assertEquals("Velez", authorProjection.getLastName());
                    Assertions.assertEquals(LocalDate.parse("2006-06-15"), authorProjection.getBirthdate());

                    // Comprobamos que el m√©todo por defecto de la proyecci√≥n est√° funcionando
                    Assertions.assertEquals("Bel√©n Velez", authorProjection.getFullName());
                })
                .verifyComplete();
    }

    @Test
    void findCountByQuery() {
        this.authorRepository.findCountByQuery("e")
                .as(StepVerifier::create)
                .assertNext(count -> Assertions.assertEquals(3, count))
                .verifyComplete();
    }

    @Test
    void findByQueryAndPageable() {
        this.authorRepository.findByQuery("e", PageRequest.of(0, 2))
                .doOnNext(authorProjection -> log.info("{}", authorProjection))
                .as(StepVerifier::create)
                .assertNext(authorProjection -> {
                    Assertions.assertEquals("Bel√©n", authorProjection.getFirstName());
                    Assertions.assertEquals("Velez", authorProjection.getLastName());
                    Assertions.assertEquals(LocalDate.parse("2006-06-15"), authorProjection.getBirthdate());
                    //default method
                    Assertions.assertEquals("Bel√©n Velez", authorProjection.getFullName());
                })
                .assertNext(authorProjection -> {
                    Assertions.assertEquals("Greys", authorProjection.getFirstName());
                    Assertions.assertEquals("Briones", authorProjection.getLastName());
                    Assertions.assertEquals(LocalDate.parse("2001-10-03"), authorProjection.getBirthdate());
                    //default method
                    Assertions.assertEquals("Greys Briones", authorProjection.getFullName());
                })
                .verifyComplete();
    }

    @Test
    void saveAuthor() {
        Author author = Author.builder()
                .firstName("Susana")
                .lastName("Alvarado")
                .birthdate(LocalDate.parse("2000-11-07"))
                .build();
        this.authorRepository.saveAuthor(author)
                .doOnNext(affectedRows -> log.info("affectedRows: {}", affectedRows))
                .as(StepVerifier::create)
                .assertNext(affectedRows -> Assertions.assertEquals(1, affectedRows))
                .verifyComplete();

        this.authorRepository.count()
                .as(StepVerifier::create)
                .expectNext(5L)
                .verifyComplete();

        this.authorRepository.findById(5)
                .doOnNext(authorRetrieved -> log.info("{}", authorRetrieved))
                .as(StepVerifier::create)
                .assertNext(authorRetrieved -> {
                    Assertions.assertNotNull(authorRetrieved.getId());
                    Assertions.assertEquals("Susana", authorRetrieved.getFirstName());
                    Assertions.assertEquals("Alvarado", authorRetrieved.getLastName());
                    Assertions.assertEquals(LocalDate.parse("2000-11-07"), authorRetrieved.getBirthdate());
                })
                .verifyComplete();

        this.authorRepository.deleteById(5)
                .then(this.authorRepository.count())
                .as(StepVerifier::create)
                .expectNext(4L)
                .verifyComplete();
    }

    @Test
    void updateCustomer() {
        this.authorRepository.findById(1)
                .doOnNext(author -> log.info("{}", author))
                .doOnNext(author -> author.setFirstName("Belencita"))
                .flatMap(author -> this.authorRepository.updateAuthor(author))
                .as(StepVerifier::create)
                .assertNext(affectedRows -> Assertions.assertEquals(1, affectedRows))
                .verifyComplete();

        this.authorRepository.findById(1)
                .doOnNext(author -> log.info("{}", author))
                .as(StepVerifier::create)
                .assertNext(author -> Assertions.assertEquals("Belencita", author.getFirstName()))
                .verifyComplete();
    }
}
````

## Creando DTOS

A continuaci√≥n mostramos los dtos creados usando `record`. Los primeros records creados son para las
consultas que realizaremos usando `criteria`, es decir consultas que ser√°n elaboradas de manera din√°mica.

````java
public record BookCriteria(String query, LocalDate publicationDate) {
}
````

````java
public record AuthorCriteria(String firstName, String lastName) {
}
````

````java
public record AuthorFilter(String query) {
}
````

````java
public record AuthorRequest(String firstName,
                            String lastName,
                            LocalDate birthdate) {
}
````

````java
public record RegisterBook(String title,
                           LocalDate publicationDate,
                           Boolean onlineAvailability,
                           List<Integer> authors) {
    public RegisterBook { // Constructor compacto
        onlineAvailability = Boolean.TRUE.equals(onlineAvailability); // Valor por defecto false si el onlineAvailability es null
    }
}
````

## Creando Servicios

Definimos la interfaz para la entidad `Author`.

````java
public interface AuthorService {
    Flux<AuthorResponse> findAllAuthors();

    Mono<AuthorProjection> findAuthorById(Integer authorId);

    Mono<Page<AuthorProjection>> getAllAuthorsToPage(String query, int pageNumber, int pageSize);

    Mono<Integer> saveAuthor(Mono<AuthorRequest> authorRequestMono);

    Mono<AuthorProjection> updateAuthor(Integer authorId, Mono<AuthorRequest> authorRequestMono);

    Mono<Boolean> deleteAuthor(Integer authorId);
}
````

Implementamos la clase de servicio `AuthorServiceImpl`.

````java

@Slf4j
@RequiredArgsConstructor
@Service
public class AuthorServiceImpl implements AuthorService {

    private final AuthorRepository authorRepository;
    private final AuthorMapper authorMapper;

    @Override
    @Transactional(readOnly = true)
    public Flux<AuthorResponse> findAllAuthors() {
        return this.authorRepository.findAll()
                .map(this.authorMapper::toAuthorResponse);
    }

    @Override
    @Transactional(readOnly = true)
    public Mono<AuthorProjection> findAuthorById(Integer authorId) {
        return this.authorRepository.findAuthorById(authorId)
                .switchIfEmpty(ApplicationExceptions.authorNotFound(authorId));
    }

    @Override
    @Transactional(readOnly = true)
    public Mono<Page<AuthorProjection>> getAllAuthorsToPage(String query, int pageNumber, int pageSize) {
        Pageable pageable = PageRequest.of(pageNumber, pageSize);
        return Mono.zip(
                this.authorRepository.findByQuery(query, pageable).collectList(),
                this.authorRepository.findCountByQuery(query),
                (data, total) -> new PageImpl<>(data, pageable, total)
        );
    }

    @Override
    @Transactional
    public Mono<Integer> saveAuthor(Mono<AuthorRequest> authorRequestMono) {
        return authorRequestMono
                .map(this.authorMapper::toAuthor)
                .flatMap(this.authorRepository::saveAuthor);
    }

    @Override
    @Transactional
    public Mono<AuthorProjection> updateAuthor(Integer authorId, Mono<AuthorRequest> authorRequestMono) {
        return this.authorRepository.findById(authorId)
                .flatMap(author -> authorRequestMono)
                .map(this.authorMapper::toAuthor)
                .doOnNext(author -> author.setId(authorId))
                .flatMap(this.authorRepository::updateAuthor)
                .flatMap(affectedRows -> this.authorRepository.findAuthorById(authorId))
                .switchIfEmpty(ApplicationExceptions.authorNotFound(authorId));
    }

    @Override
    @Transactional
    public Mono<Boolean> deleteAuthor(Integer authorId) {
        return this.authorRepository.findById(authorId)
                .flatMap(author -> this.authorRepository.deleteAuthorById(authorId))
                .switchIfEmpty(ApplicationExceptions.authorNotFound(authorId));
    }
}
````

## üìÑ Anotaci√≥n `@Transactional` en aplicaciones reactivas con Spring WebFlux y R2DBC

### üéØ Objetivo General

Verificar y documentar el uso de la anotaci√≥n `@Transactional` en aplicaciones reactivas, espec√≠ficamente en un
proyecto con:

- Spring WebFlux
- Spring Data R2DBC
- PostgreSQL

Se sabe de antemano que:

> ‚úÖ La anotaci√≥n `@Transactional` funciona correctamente en aplicaciones reactivas para delimitar transacciones que
> modifican la base de datos (`INSERT`, `UPDATE`, `DELETE`).

‚ö†Ô∏è El enfoque de estas pruebas fue comprobar espec√≠ficamente que `@Transactional(readOnly = true)` efectivamente
bloquea escrituras.

Para ello se dise√±aron experimentos que forzaran un `INSERT` bajo una transacci√≥n marcada como de `solo lectura`.

### ‚öôÔ∏è Entorno de Pruebas

- Spring Boot: 3.5.3
- Spring Data R2DBC: versi√≥n incluida en Spring Boot 3.5.3
- Driver R2DBC: io.r2dbc:r2dbc-postgresql
- Base de datos: PostgreSQL
- Configuraci√≥n adicional:
    - √önicamente propiedades de conexi√≥n en application.yml
    - Logs SQL habilitados
    - Sin TransactionManager personalizado

### üß™ Pruebas realizadas

A continuaci√≥n se detallan las pruebas y resultados.

### üü¢ 1Ô∏è‚É£ Prueba `SIN` `@Transactional(readOnly = true)`

üìå C√≥digo del m√©todo

````java

@Override
public Flux<AuthorResponse> findAllAuthors() {
    return this.authorRepository.save(Author.builder()
                    .firstName("Ale")
                    .lastName("Flo")
                    .birthdate(LocalDate.parse("2025-05-06"))
                    .build())
            .doOnNext(author -> log.info("{}", author))
            .flatMapMany(author -> this.authorRepository.findAll())
            .map(this.authorMapper::toAuthorResponse);
}
````

üìã Resultado observado

- Se ejecut√≥ correctamente un INSERT.
- Luego se ejecut√≥ el SELECT de todos los autores.
- La respuesta incluy√≥ el autor reci√©n insertado.
- No se produjo ning√∫n error.

Log en el Ide

````bash
io.r2dbc.postgresql.PARAM                : Bind parameter [0] to: Ale
io.r2dbc.postgresql.PARAM                : Bind parameter [1] to: Flo
io.r2dbc.postgresql.PARAM                : Bind parameter [2] to: 2025-05-06
io.r2dbc.postgresql.QUERY                : Executing query: INSERT INTO authors (first_name, last_name, birthdate) VALUES ($1, $2, $3) RETURNING id
d.m.r.a.service.impl.AuthorServiceImpl   : Author(id=5, firstName=Ale, lastName=Flo, birthdate=2025-05-06)
io.r2dbc.postgresql.QUERY                : Executing query: SELECT authors.* FROM authors
````

Log en el cliente

````bash
$ curl -v http://localhost:8080/api/v1/authors/stream
>
< HTTP/1.1 200 OK
< transfer-encoding: chunked
< Content-Type: text/event-stream;charset=UTF-8
<
data:{"id":1,"firstName":"Milagros","lastName":"D√≠az","birthdate":"2006-06-15"}

data:{"id":2,"firstName":"Lesly","lastName":"√Åguila","birthdate":"1995-06-09"}

data:{"id":3,"firstName":"Kiara","lastName":"Lozano","birthdate":"2001-10-03"}

data:{"id":4,"firstName":"Briela","lastName":"Cirilo","birthdate":"1997-09-25"}

data:{"id":5,"firstName":"Ale","lastName":"Flo","birthdate":"2025-05-06"}
````

‚úÖ Conclusi√≥n:
> El m√©todo `sin readOnly` permite insertar datos normalmente.

### üîµ 2Ô∏è‚É£ Prueba `CON` `@Transactional(readOnly = true)`

üìå C√≥digo del m√©todo

````java

@Override
@Transactional(readOnly = true)
public Flux<AuthorResponse> findAllAuthors() {
    return this.authorRepository.save(Author.builder()
                    .firstName("Ale")
                    .lastName("Flo")
                    .birthdate(LocalDate.parse("2025-05-06"))
                    .build())
            .doOnNext(author -> log.info("{}", author))
            .flatMapMany(author -> this.authorRepository.findAll())
            .map(this.authorMapper::toAuthorResponse);
}
````

üìã Resultado observado

- La transacci√≥n inici√≥ en modo `READ ONLY`.
- PostgreSQL `bloque√≥ la escritura`.
- Se produjo un `rollback` autom√°tico.
- La llamada devolvi√≥ `error HTTP 500`.

Log en el ide

````bash
DEBUG 17820 --- io.r2dbc.postgresql.QUERY                : Executing query: BEGIN READ ONLY
DEBUG 17820 --- io.r2dbc.postgresql.PARAM                : Bind parameter [0] to: Ale
DEBUG 17820 --- io.r2dbc.postgresql.PARAM                : Bind parameter [1] to: Flo
DEBUG 17820 --- io.r2dbc.postgresql.PARAM                : Bind parameter [2] to: 2025-05-06
DEBUG 17820 --- io.r2dbc.postgresql.QUERY                : Executing query: INSERT INTO authors (first_name, last_name, birthdate) VALUES ($1, $2, $3) RETURNING id
DEBUG 17820 --- io.r2dbc.postgresql.QUERY                : Executing query: ROLLBACK
ERROR 17820 --- a.w.r.e.AbstractErrorWebExceptionHandler : [35475d0d-4]  500 Server Error for HTTP GET "/api/v1/authors/stream"

org.springframework.dao.DataAccessResourceFailureException: executeMany; SQL [INSERT INTO authors (first_name, last_name, birthdate) VALUES ($1, $2, $3)]; no se puede ejecutar INSERT en una transacci√≥n de s√≥lo lectura
````

Log en el cliente

````bash
$ curl -v http://localhost:8080/api/v1/authors/stream | jq
>
< HTTP/1.1 500 Internal Server Error
< Content-Type: application/json
< Content-Length: 319
<
{
  "timestamp": "2025-06-29T00:35:33.603+00:00",
  "path": "/api/v1/authors/stream",
  "status": 500,
  "error": "Internal Server Error",
  "requestId": "35475d0d-4",
  "message": "executeMany; SQL [INSERT INTO authors (first_name, last_name, birthdate) VALUES ($1, $2, $3)]; no se puede ejecutar INSERT en una transacci√≥n de s√≥lo lectura"
}
````

‚úÖ Conclusi√≥n:
> `@Transactional(readOnly = true)` se traduce correctamente en `BEGIN READ ONLY` y
> `PostgreSQL bloquea cualquier INSERT`.

### ‚ú® Conclusiones finales

1. La anotaci√≥n `@Transactional` en aplicaciones reactivas con `Spring WebFlux` y `R2DBC` funciona correctamente para
   delimitar transacciones que modifican la base de datos (`INSERT`, `UPDATE`, `DELETE`).
2. La anotaci√≥n `@Transactional(readOnly = true)` tambi√©n funciona correctamente, al iniciar la transacci√≥n en modo
   `READ ONLY` en `PostgreSQL`.
3. Al intentar insertar datos dentro de una transacci√≥n de solo lectura, `PostgreSQL` bloquea la operaci√≥n con un error
   claro, y `Spring` realiza un `rollback autom√°tico`.
4. Estas pruebas demuestran de forma emp√≠rica que el soporte de `readOnly=true` est√° operativo en `Spring Boot 3.5.x`
   sin necesidad de configuraciones adicionales.

üîî Importante

- `R2DBC + PostgreSQL`: la restricci√≥n de solo lectura la hace la base de datos.
- `JPA + Hibernate`: la restricci√≥n de solo lectura la hace Hibernate en la sesi√≥n.
- No todas las bases de datos se comportan igual.
- Siempre es buena pr√°ctica declarar la intenci√≥n con `readOnly = true`.
